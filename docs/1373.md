# 递归层

> 原文：<https://www.javatpoint.com/keras-recurrent-layers>

## RNN

```

keras.engine.base_layer.wrapped_fn()

```

RNN 层充当递归层的基类。

**论据**

*   **cell:** 可以定义为 RNN cell 的一个实例，这是一个类，它构成:同样，可能存在这样一种可能性，即该单元是 RNN 单元实例的列表；然后，在这种情况下，单元在 RNN 被一个接一个地堆叠，导致堆叠的 RNN 的有效实现。
    *   一个**调用(input_at_t，states_at_t)** 方法返回 **(output_at_t，states_at_t_plus_1)。**它可以选择一个**常量**参数，这将在下面“传递外部常量的注意事项”一节中更简要地解释。
    *   一个 **state_size** 属性可以简单地定义为一个整数(状态整数)或者一个整数列表/元组(每个状态一个大小)。在单个整数的情况下，它充当循环状态的大小，该大小必须与输出单元格的大小相似。
    *   一个 **output_size** 属性，可以被称为单个整数或一个表征输出形状的 TensorShape。在向后兼容的情况下，当属性对于单元不可用时，该值有可能通过其在 **state_size** 上的初始元素来推断。Also, there may be a possibility where the cell is a list of RNN cell instances; then, in that case, the cell gets stacked one after another in the RNN, leading to an efficient implementation of the stacked RNN.
*   **return_sequences:** 它是一个布尔值，描述了输出序列或完整序列中要返回的最后一个输出。
*   **return_states:** 它也是布尔型的，用于描述最后一个状态是否应该在输出之外返回。
*   **go _ backed:**为布尔型，默认为 False。如果设置为真，它将反向处理输入序列，并以相反的顺序返回。
*   **有状态:**为布尔型，默认为 False。如果“有状态”设置为“真”，则对于该批次中第 i <sup>个</sup>索引处的每个样本，最后一个状态将用作下一批次中第 i <sup>个</sup>索引处样本的初始状态。
*   **展开:**为布尔型(默认为假)。如果这是真的，那么它要么展开网络，要么使用符号循环。RNN 可以加快展开速度，即使它是内存密集型的，因为它更适合较短的序列。
*   **input_dim:** 它是一个描述输入维度的整数。当此图层将用作模型中的初始图层时，将使用 input_shape 参数。
*   **input_length:** 描述输入序列的长度，该长度在恒定时指定。当我们首先想要将其连接到上游的**展平**然后连接到**密集**层时使用它，因为它有助于计算密集层的输出形状。如果循环层不是模型中的初始层，则必须通过 **input_shape** 在第一层指定输入的长度

**输入形状**

它是形状的 3D 张量**(批处理大小，时间步长，输入尺寸)**。

**输出形状**

*   如果**返回 _ 状态**:张量列表，那么第一个张量为输出，其余为最后一个状态，每个形状 **(batch_size，units)** 如例；对于 RNN 和 GRU，张量的状态数是 1，对于 LSTM 是 2。
*   如果**返回 _ 序列** : 3D，那么张量的形状将是**(批量大小，时间步长，单位)**，否则如果是 2D，那么形状将是**(批量大小，单位)**。

**遮蔽**

该层支持屏蔽，以多个时间步长输入数据。**嵌入**层与**蒙版 _ 零**参数一起使用，该参数设置为**真**，用于将蒙版引入数据。

**关于在 RNNs 中使用状态的注意事项**

如果您将 RNN 层设置为“有状态”，则意味着在单个批次中为样本计算的状态将再次用作下一批次样本的初始状态。这意味着在不同连续批次的样品之间进行一对一的映射。

要启用状态性，您需要在构造器层内部指定 **stateful=True** ，然后为模型指定固定的批处理大小，这是通过将 if 顺序模型: **batch_input_shape=(…)** 传递给模型中的初始(第一)层来完成的，否则对于包含 1 个或多个输入层的任何功能模型: **batch_shape=(…)** 传递给模型中的所有第一层。输入的预期形状包括作为整数元组的批次大小，例如 **(32，10，100)** 并在调用 fit()时指定 **shuffle=False** 。

还有，需要调用**。reset_states()** 在指定的图层或整个模型上，如果您愿意重置模型的状态。

**关于指定 RNNs 初始状态的说明**

RNN 层的初始状态可以通过用 **initial_state** 关键字参数调用它们来象征性地指定，这样它的值必须是描述 RNN 层初始状态的张量或张量列表。

RNN 层的初始状态可以通过用 **states** 关键字参数调用 **reset_states** 来数字指定，因此其值必须是描述 RNN 层初始状态的 numpy 数组或数组列表。

**关于向 RNNs 传递外部常数的注意事项**

利用 **RNN 的**常量**关键字参数，可以将外部常量传递给单元格。__ 调用 __** 和 **RNN .调用需要**单元格的**方法.调用**方法接受相同的关键字参数**常量**。这些常数用于调节附加静态输入(不随时间变化)上的细胞转化。

**例**

```

# First, let's define an RNN Cell, as a layer subclass.

class MinimalRNNCell(keras.layers.Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
        self.built = True

    def call(self, inputs, states):
        prev_output = states[0]
        h = K.dot(inputs, self.kernel)
        output = h + K.dot(prev_output, self.recurrent_kernel)
        return output, [output]

# Let's use this cell in a RNN layer:

cell = MinimalRNNCell(32)
x = keras.Input((None, 5))
layer = RNN(cell)
y = layer(x)

# Here's how to use the cell to build a stacked RNN:

cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
x = keras.Input((None, 5))
layer = RNN(cells)
y = layer(x)

```

## 简单网络

```

keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)

```

它是一个完全连接的层，其输出被发送回输入端。

**论据**

*   **单位:**可以定义为代表输出空间维度的正整数。
*   **激活:**是要使用的激活函数，默认为双曲正切 **(tanh)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **use_bias:** 可以定义为一个布尔值，用于描述图层是否使用偏置向量。
*   **内核初始化器:**它是指用于线性变换输入的**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它是**递归 _ 核**权重矩阵的初始值设定项，应该在线性变换递归状态时使用。
*   **bias_initializer:** 表示用于偏置向量的初始化器。
*   **核 _ 正则化器:**是指在**核**权重矩阵上实现的正则化函数。
*   **递归正则化器:**是指应用于**递归核**权重矩阵的正则化函数。
*   **bias _ regulator:**理解为正则化函数，应用于偏置向量。
*   **activity _ regulator:**是应用于激活(图层输出)的正则化函数。
*   **内核 _ 约束:**可以定义为应用于**内核**的约束函数
*   **bias_constraint:** 可以定义为对偏置向量执行的约束函数。
*   **递归 _ 约束:**可以定义为应用于**递归 _ 核**权重矩阵的约束函数。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。
*   **return_sequences:** 它指的是一个布尔值，用于描述输出序列或完整序列中要返回的最后一个输出。
*   **return_states:** 它指的是一个布尔值，它描述了最后一个状态是否应该在输出之外返回。
*   **go _ backward:**指的是一个布尔值，默认设置为 False。在这种情况下，如果它是真的，那么它会反向处理输入序列，并返回反向序列。
*   **有状态:**指的是一个布尔值，默认为 False。如果为真，则对于 i <sup>第</sup>指数批次中的每个样品，最后一个状态将用作下一批次中 i <sup>第</sup>指数样品的初始状态。
*   **展开:**为布尔型(默认为假)。如果这是真的，那么它要么展开网络，要么使用符号循环。RNN 可以加快展开速度，即使它是内存密集型的，因为它更适合较短的序列。

## 苏军总参谋部情报总局

```

keras.layers.GRU(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False)

```

它被称为门控递归单元，并带有两个变体，例如基于 1406.1078v3 的默认单元由一个重置门组成，该重置门在矩阵乘法之前应用于隐藏状态，另一个重置门基于原始的 1406.1078v1，顺序颠倒。

另一个版本(第二个)与允许中央处理器推理的 CuDNNGRU(仅限图形处理器)非常匹配。因此，可以说它包含了对**核**和**递归 _ 核**的不同偏见，所以最好使用**“reset _ after”= True**和**递归 _ activation =“sigmoid”**。

**论据**

*   **单位:**可以定义为表示输出空间维度的正整数。
*   **激活:**指要使用的激活函数，默认为双曲正切 **(tanh)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **recursive _ activation:**是一个用于循环步骤的激活函数，默认情况下为硬 sigmoid **(硬 sigmoid)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **use_bias:** 可以定义为一个布尔值，用于描述图层是否使用偏置向量。
*   **内核初始化器:**它表示用于线性变换输入的**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它是指**递归 _ 核**权重矩阵的初始值设定项，该权重矩阵应该在线性变换递归状态时使用。
*   **bias_initializer:** 可以定义为偏置向量的初始值设定项。
*   **核 _ 正则化器:**是指在**核**权重矩阵上实现的正则化函数。
*   **递归 _ 正则化器:**是指应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**是指正则化函数，对偏置向量执行。
*   **activity _ regulator:**表示应用于激活(图层输出)的正则化函数。
*   **内核 _ 约束:**表示约束函数，应用于**内核**
*   **bias_constraint:** 指应用于偏置向量的约束函数。
*   **递归 _ 约束:**是指在**递归 _ 核**权重矩阵上实现的约束函数。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。
*   **实现:**是一种实现方式，可以是 1，也可以是 2。在模式 1 中，操作将被构造为大量较小的点积和加法，而在模式 2 中，它将把它们批处理为一些大的操作。这些模式将展示不同硬件和应用的不同性能特征。
*   **return_sequences:** 它指的是一个布尔值，用于描述输出序列或完整序列中要返回的最后一个输出。
*   **return_states:** 它也指一个布尔值，用于描述最后一个状态是否应该在输出之外返回。
*   **go _ backed:**指布尔型，默认为 False。在这种情况下，如果它是真的，那么它会反向处理输入序列，并返回反向序列。
*   **有状态:**可以定义为布尔值，默认为 False。如果为真，则对于该批次中第 i <sup>个</sup>索引处的每个样本，最后一个状态将被用作下一批次中第 i <sup>个</sup>索引处的样本的初始状态。
*   **展开:**可以定义为布尔值(默认为 False)。如果这是真的，那么它要么展开网络，要么使用符号循环。RNN 可以加快展开速度，即使它是内存密集型的，因为它更适合较短的序列。
*   **reset_after:** 这是一个 GRU 约定，描述了在矩阵乘法之前还是之后应用复位门。False =“之前”(默认)，True =“之后”(兼容 CuDNN)。

## -什么

```

keras.layers.LSTM(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)

```

它被称为长短期记忆，由霍克瑞特尔于 1997 年提出。

**论据**

*   **单位:**表示输出空间维度的正整数。
*   **激活:**可以定义为要使用的激活函数，默认为双曲正切 **(tanh)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **recursive _ activation:**是一个用于循环步骤的激活函数，默认情况下为硬 sigmoid **(硬 sigmoid)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **use_bias:** 是指为图层描绘是否使用偏置向量的布尔型。
*   **内核初始化器:**它是指用于线性变换输入的**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它是指**递归 _ 核**权重矩阵的初始值设定项，该权重矩阵应该在线性变换递归状态时使用。
*   **bias_initializer:** 表示偏置向量的初始化器。
*   **unit _ 忘我 _bias:** 表示布尔值，如果设置为 True，初始化时忘我门的偏置会加 1。此外，它将强制执行 **bias_initializer=“零”**。
*   **核 _ 正则化器:**它指的是一个正则化器函数，它被应用于**核**权重矩阵。
*   **递归 _ 正则化器:**是指应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**是指正则化函数，正在偏置向量上实现。
*   **activity _ regulator:**是指应用于激活(图层输出)的正则化函数。
*   **内核 _ 约束:**是指在**内核**上执行的约束函数
*   **bias_constraint:** 指的是一个约束函数，它被应用于偏置向量。
*   **递归 _ 约束:**是应用于**递归 _ 核**权重矩阵的约束函数。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。
*   **实现:**是一种实现方式，可以是 1，也可以是 2。在模式 1 中，操作将被构造为大量较小的点积和加法，而在模式 2 中，它将把它们批处理为一些大的操作。这些模式将展示不同硬件和应用的不同性能特征。
*   **return_sequences:** 它指的是一个布尔值，用于描述输出序列或完整序列中要返回的最后一个输出。
*   **return_states:** 也指布尔型，表示除了输出之外，最后一个状态是否应该返回。
*   **go _ backward:**可以定义为布尔值，默认为 False。在这种情况下，如果它是真的，那么它会反向处理输入序列，并返回反向序列。
*   **有状态:**可以理解为 Boolean，默认为 False。如果为真，则对于该批次中第 i <sup>个</sup>索引处的每个样本，最后一个状态将被用作下一批次中第 i <sup>个</sup>索引处的样本的初始状态。
*   **展开:**表示布尔值(默认为假)。如果这是真的，那么它要么展开网络，要么使用符号循环。RNN 可以加快展开速度，即使它是内存密集型的，因为它更适合较短的序列。

## convtomm 2d

```

keras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0)

```

它是卷积 LSTM 层，与 LSTM 层相同，只是输入和循环变换都是卷积的。

**论据**

*   **滤波器:**它是指一个整数，表示输出空间维度或卷积中存在的输出滤波器总数。
*   **kernel_size:** 可以是整数，也可以是 n 个整数的元组/列表，代表卷积窗的维数。
*   **步距:**代表卷积步距的不是整数就是 n 个整数的元组/列表。如果我们指定任何步幅值！=1，与指定**膨胀率**值不兼容！=1.
*   **填充:**【有效】**或**【相同】**之一(区分大小写)。**
*   **data_format:** 是“channels_last”或“channels_first”的字符串，是输入维度的顺序。这里的**“channels _ last”**涉及到输入形状**(批次、时间、...，通道)**，而**“通道 _ 第一”**涉及输入形状**(批次、时间、通道、...)**。默认为**图像 _ 数据 _ 格式**值，可在 **~/的 Keras 配置中找到。keras/keras.json** 。如果您在该文件夹中找不到它，那么它就位于“channels_last”。
*   **膨胀率:**它可以是一个整数或 n 个整数的元组/列表，与用于膨胀卷积的膨胀率相关。如果我们指定任何步幅值！=1，与指定**膨胀率**值不兼容！=1.
*   **激活:**是要使用的激活功能。当未指定任何内容时，则默认为线性激活 **a(x)= x** ，或者我们可以说未应用激活功能。
*   **recurrent_activation:** 是用于循环步骤的激活函数。
*   **use_bias:** 是指为图层定义是否使用偏置向量的布尔值。
*   **内核 _ 初始化器:**可以定义为**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**可以理解为**递归 _ 核**权重矩阵的初始值设定项，该权重矩阵应该在线性变换递归状态时使用。
*   **bias_initializer:** 指用于偏置向量的初始化器。
*   **unit _ 忘我 _bias:** 可以定义为布尔型，如果设置为 True，初始化时忘我门的偏置加 1。此外，它将强制执行 **bias_initializer=“零”**。
*   **核 _ 正则化器:**是指正则化器函数，在**核**权重矩阵上实现。
*   **递归 _ 正则化器:**是指应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**是指正则化函数，应用于偏置向量。
*   **activity _ regulator:**表示应用于激活的正则化函数(即图层的输出)。
*   **kernel_constraint:** 指应用于核矩阵的约束函数。
*   **递归 _ 约束:**定义为应用于**递归 _ 核**权重矩阵的约束函数。
*   **bias_constraint:** 表示应用于偏置向量的约束函数。
*   **return_sequences:** 它指的是一个布尔值，用于描述输出序列或完整序列中要返回的最后一个输出。
*   **go _ backward:**指的是一个布尔值，默认设置为 False。在这种情况下，如果它是真的，那么它会反向处理输入序列，并返回反向序列。
*   **有状态:**表示布尔值，默认为假。如果为真，则对于该批次中第 i <sup>个</sup>索引处的每个样本，最后一个状态将被用作下一批次中第 i <sup>个</sup>索引处的样本的初始状态。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。

**输入形状**

如果 **data_format** 为**“channels _ first”**，那么 5D 张量的输入形状为**(样本、时间、通道、行、列)**否则如果 **data_format** 为**“channels _ last”**5D 张量的输入形状为**(样本、时间、行、列、通道)。**

**输出形状**

*   如果**返回 _ 序列**
    *   如果**数据格式**是**“通道 _ 第一”**，则 5D 张量的输出形状将是**(样本、时间、过滤器、输出 _ 行、输出 _ 列)**。
    *   如果**数据格式**是**“通道 _ 最后”**，则 5D 张量的输出形状将是**(样本、时间、输出 _ 行、输出 _ 列、过滤器)**。
*   其他
    *   如果**数据格式**是**“通道 _ 第一”**，则 4D 张量的输出形状将是**(样本、滤波器、输出 _ 行、输出 _ 列)**。
    *   如果**数据格式**是**“通道 _ 最后”**，则 4D 张量的输出形状将是**(采样、output_row、output_col、滤波器)**，其中 o_row 和 o_col 取决于滤波器和填充形状。

**提升**

*   **值错误:**在构造函数参数无效的情况下引发。

## convlstm 2 dcell

```

keras.layers.ConvLSTM2DCell(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)

```

这是一个用于 ConvLSTM2D 层的单元类。

**论据**

*   **滤波器:**它是指一个整数，表示输出空间维度或卷积中存在的输出滤波器总数。
*   **kernel_size:** 可以是整数，也可以是 n 个整数的元组/列表，代表卷积窗的维数。
*   **步距:**它可以是整数，也可以是表示卷积步距的 n 个整数的元组/列表。如果我们指定任何步幅值！=1，与指定**膨胀率**值不兼容！=1.
*   **填充:**【有效】**或**【相同】**之一(区分大小写)。**
*   **数据 _ 格式:**可以定义为**“channels _ last”**或者**“channels _ first”**的字符串，是输入维度的顺序。默认为 **image_data_format** 值，可在**处的 Keras 配置中找到。keras/keras.json** 。如果您在该文件夹中找不到它，那么它就位于“channels_last”。
*   **膨胀率:**它可以是一个整数或 n 个整数的元组/列表，与用于膨胀卷积的膨胀率相关。如果我们指定任何步幅值！=1，与指定**膨胀率**值不兼容！=1.
*   **激活:**是指要使用的激活功能。当未指定任何内容时，则默认为线性激活 **a(x)= x** ，或者我们可以说未应用激活功能。
*   **recurrent_activation:** 是用于循环步骤的激活函数。
*   **use_bias:** 可以定义为一个布尔型，用于描述一个图层是否使用偏置向量。
*   **内核初始化器:**它指的是**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它是指**递归 _ 核**权重矩阵的初始值设定项，该权重矩阵应该在线性变换递归状态时使用。
*   **bias_initializer:** 可以定义为偏置向量的初始值设定项。
*   **unit _ 忘我 _bias:** 可以定义为 Boolean，如果设置为 True，初始化时忘我门的偏置加 1。此外，它将强制执行 **bias_initializer=“零”**。
*   **核 _ 正则化器:**可以定义为正则化函数，应用于**核**权重矩阵。
*   **递归 _ 正则化器:**可以定义为应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**是指正则化函数，应用于偏置向量。
*   **kernel_constraint:** 指应用于核矩阵的约束函数。
*   **递归 _ 约束:**是指应用于**递归 _ 核**权重矩阵的约束函数。
*   **bias_constraint:** 可以定义为一个约束函数，正在应用于偏置向量。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。

## 简单全部

```

keras.layers.SimpleRNNCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)

```

它是 SimpleRNN 的一个单元类。

**论据**

*   **单位:**表示输出空间维度的正整数。
*   **激活:**是要使用的激活功能。当未指定任何内容时，则默认为线性激活 **a(x) = x** ，或者我们可以说未应用激活功能。
*   **use_bias:** 可以定义为布尔型，用于描述图层是否使用偏置向量。
*   **内核初始化器:**是指**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它是指**递归 _ 核**权重矩阵的初始值设定项，该权重矩阵应该在线性变换递归状态时使用。
*   **bias_initializer:** 它向初始化器指示偏置向量。
*   **核 _ 正则化器:**可以定义为正则化函数，正在**核**权重矩阵上实现。
*   **递归 _ 正则化器:**可以定义为应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**指正则化函数，应用于偏置向量。
*   **kernel_constraint:** 指应用于核矩阵的约束函数。
*   **递归 _ 约束:**可以定义为在**递归 _ 核**权重矩阵上执行的约束函数。
*   **bias_constraint:** 是应用于偏置向量的约束函数。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。

## GRUCell

```

keras.layers.GRUCell(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, reset_after=False)

```

它是 GRU 层的一个单元类。

**论据**

*   **单位:**表示输出空间维度的正整数。
*   **激活:**可以理解为要使用的激活函数，默认为双曲正切 **(tanh)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **recursive _ activation:**可以定义为用于循环步骤的激活函数，默认为硬 sigmoid **(硬 sigmoid)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **use_bias:** 指的是一个布尔值，它描述了一个图层是否使用偏置向量。
*   **内核初始化器:**它是指用于线性变换输入的**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它可以定义为**递归 _ 核**权重矩阵的初始值设定项，该权重矩阵应该在线性变换递归状态时使用。
*   **bias_initializer:** 是指偏置向量的初始化器。
*   **核 _ 正则化器:**可以定义为正则化函数，应用于**核**权重矩阵。
*   **递归 _ 正则化器:**是指应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**是正则化函数，应用于偏置向量。
*   **内核 _ 约束:**是指应用于**内核**的约束函数
*   **bias_constraint:** 可以理解为一个约束函数，正在应用于偏置向量。
*   **递归 _ 约束:**表示应用于**递归 _ 核**权重矩阵的约束函数。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。
*   **实现:**是一种实现方式，可以是 1，也可以是 2。在模式 1 中，操作将被构造为大量较小的点积和加法，而在模式 2 中，它将把它们批处理为一些大的操作。这些模式将展示不同硬件和应用的不同性能特征。
*   **reset_after:** 这是一个 GRU 约定，描述了在矩阵乘法之前还是之后应用复位门。False =“之前”(默认)，True =“之后”(兼容 CuDNN)。

## LSTMCell

```

keras.layers.LSTMCell(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2)

```

它被称为 LSTM 层的单元类。

**论据**

*   **单位:**表示输出空间维度的正整数。
*   **激活:**是要使用的激活函数，默认为双曲正切 **(tanh)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **recursive _ activation:**是一个用于循环步骤的激活函数，默认情况下为硬 sigmoid **(硬 sigmoid)** 。如果**无**通过，则表示没有应用任何东西(即“线性”激活 a(x) = x)。
*   **use_bias:** 指的是一个布尔值，它描述了一个图层是否使用偏置向量。
*   **内核初始化器:**它是指用于线性变换输入的**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它向初始值设定项指示在线性变换递归状态时应该使用的**递归 _ 核**权重矩阵。
*   **bias_initializer:** 是指偏置向量的初始化器。
*   **unit _ 忘我 _bias:** 为布尔型，如果设置为 True，初始化时忘我门的偏置加 1。此外，它将强制执行 **bias_initializer=“零”**。
*   **核 _ 正则化器:**它指的是一个正则化器函数，它被应用于**核**权重矩阵。
*   **递归 _ 正则化器:**是指正在**递归 _ 核**权重矩阵上实现的正则化器函数。
*   **bias _ regulator:**指正则化函数，应用于偏置向量。
*   **内核 _ 约束:**是指应用于**内核**的约束函数
*   **bias_constraint:** 指应用于偏置向量的约束函数。
*   **递归 _ 约束:**是应用于**递归 _ 核**权重矩阵的约束函数。
*   **drop:**它是一个介于 0 和 1 之间的浮点数，描述了对输入进行线性变换时要丢弃的单位分数的总数。
*   **recursive _ drop:**它是一个介于 0 和 1 之间的浮点数，它描述了线性变换循环状态所需丢弃的单位分数的总数。
*   **实现:**是一种实现方式，可以是 1，也可以是 2。在模式 1 中，操作将被构造为大量较小的点积和加法，而在模式 2 中，它将把它们批处理为一些大的操作。这些模式将展示不同硬件和应用的不同性能特征。

## CuDNNGRU

```

keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)

```

它是 GRU 最快的实现之一，由 CuDNN 支持，被发现只能在带有 TensorFlow 后端的 GPU 上运行。

**论据**

*   **单位:**表示输出空间维度的正整数。
*   **内核初始化器:**它可以被定义为用于线性变换输入的**内核**权重矩阵的初始化器。
*   **递归 _ 初始值设定项:**它可以定义为**递归 _ 核**权重矩阵的初始值设定项，该权重矩阵应该在线性变换递归状态时使用。
*   **bias_initializer:** 可以定义为偏置向量的初始值设定项。
*   **核 _ 正则化器:**是指正则化函数，应用于**核**权重矩阵。
*   **递归 _ 正则化器:**是指应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**指正则化函数，应用于偏置向量。
*   **activity _ regulator:**是指应用于激活(图层输出)的正则化函数。
*   **核 _ 约束:**是应用于**核**的约束函数
*   **bias_constraint:** 是应用于偏置向量的约束函数。
*   **递归 _ 约束:**是应用于**递归 _ 核**权重矩阵的约束函数。
*   **return_sequences:** 它是一个布尔值，描述了输出序列或完整序列中要返回的最后一个输出。
*   **return_states:** 它也是布尔型的，用于描述最后一个状态是否应该在输出之外返回。
*   **有状态:**为布尔型，默认为 False。如果为真，则对于该批次中第 i <sup>个</sup>索引处的每个样本，最后一个状态将被用作下一批次中第 i <sup>个</sup>索引处的样本的初始状态。

## cudnnlstm(英国作家)

```

keras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)

```

这是 CuDNN 支持的最快的 LSTM 实现，已经发现只能在带有 TensorFlow 后端的 GPU 上运行。

**论据**

*   **单位:**表示输出空间维度的正整数。
*   **内核初始化器:**是**内核**权重矩阵的初始化器，用于线性变换输入。
*   **递归 _ 初始值设定项:**它是**递归 _ 核**权重矩阵的初始值设定项，应该在线性变换递归状态时使用。
*   **bias_initializer:** 是一个偏置向量的初始化器。
*   **unit _ 忘我 _bias:** 为布尔型，如果设置为 True，初始化时忘我门的偏置加 1。此外，它将强制执行 **bias_initializer=“零”**。
*   **核 _ 正则化器:**是一个正则化函数，应用于**核**权重矩阵。
*   **递归 _ 正则化器:**是应用于**递归 _ 核**权重矩阵的正则化函数。
*   **bias _ regulator:**是正则化函数，应用于偏置向量。
*   **activity _ regulator:**是应用于激活(图层输出)的正则化函数。
*   **核 _ 约束:**是应用于**核**的约束函数
*   **bias_constraint:** 是应用于偏置向量的约束函数。
*   **递归 _ 约束:**是应用于**递归 _ 核**权重矩阵的约束函数。
*   **return_sequences:** 它是一个布尔值，描述了输出序列或完整序列中要返回的最后一个输出。
*   **return_states:** 它也是布尔型的，用于描述最后一个状态是否应该在输出之外返回。

**有状态:**为布尔型，默认为 False。如果为真，则对于该批次中第 i <sup>个</sup>索引处的每个样本，最后一个状态将被用作下一批次中第 i <sup>个</sup>索引处的样本的初始状态。

* * *