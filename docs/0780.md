# 机器学习算法

> 原文:[https://www.javatpoint.com/machine-learning-algorithms](https://www.javatpoint.com/machine-learning-algorithms)

机器学习算法是能够从数据中学习隐藏模式，预测输出，并根据自己的经验提高性能的程序。对于不同的任务，机器学习中可以使用不同的算法，例如简单的线性回归可以用于预测问题**s 像**股市预测，**和【tasks 算法可以用于分类问题。**

 **在本主题中，我们将看到一些流行且最常用的[机器学习](https://www.javatpoint.com/machine-learning)算法的概述，以及它们的用例和类别。

## 机器学习算法的类型

机器学习算法可以大致分为三种类型:

1.  **监督学习算法**
2.  **无监督学习算法**
3.  **强化学习算法**

下图说明了不同的 ML 算法以及类别:

![Machine Learning Algorithms](../Images/ced2e7bbb2118c23ad3ef51e1716cd55.png)

### 1)监督学习算法

监督学习是机器学习的一种，其中机器需要外部监督来学习。使用标记数据集训练监督学习模型。训练和处理完成后，通过提供样本测试数据来检查模型是否预测了正确的输出，从而对模型进行测试。

监督学习的目标是将输入数据映射到输出数据。有监督的学习是建立在监督的基础上的，和学生在老师的监督下学习东西是一样的。监督学习的例子是**垃圾邮件过滤**。

监督学习可以进一步分为两类问题:

*   [分类](https://www.javatpoint.com/classification-algorithm-in-machine-learning)
*   [回归](https://www.javatpoint.com/regression-analysis-in-machine-learning)

一些流行的监督学习算法的例子是简单线性回归、决策树、逻辑回归、KNN 算法等。[阅读更多..](https://www.javatpoint.com/supervised-machine-learning)

### 2)无监督学习算法

这是一种机器学习，其中机器不需要任何外部监督来从数据中学习，因此称为无监督学习。无监督模型可以使用未分类或未归类的未标记数据集来训练，算法需要在没有任何监督的情况下对该数据进行操作。在无监督学习中，模型没有预定义的输出，它试图从海量数据中找到有用的见解。这些用于解决关联和聚类问题**。因此，它可以进一步分为两种类型:**

*   [聚类](https://www.javatpoint.com/clustering-in-machine-learning)
*   联合

一些无监督学习算法的例子有 **K-means 聚类、Apriori 算法、Eclat 等。** [阅读更多..](https://www.javatpoint.com/unsupervised-machine-learning)

### 3)强化学习

在强化学习中，代理通过产生动作与其环境交互，并在反馈的帮助下学习。反馈以奖励的形式给予代理人，比如每做一个好的动作，他得到一个正的奖励，每做一个不好的动作，他得到一个负的奖励。没有向代理人提供监督。**强化学习采用 Q-Learning 算法**。[阅读更多…](https://www.javatpoint.com/reinforcement-learning)

## 流行机器学习算法列表

1.  **线性回归算法**
2.  **逻辑回归算法**
3.  **决策树**
4.  **SVM**
5.  **天真贝**
6.  kn
7.  **K 均值聚类**
8.  **随机森林**
9.  **先验**
10.  **PCA**

### 1.线性回归

线性回归是用于预测分析的最流行和最简单的机器学习算法之一。这里**预测分析**定义了对某事物的预测，线性回归对*连续数字*进行预测，如**工资、年龄等。**

它显示了因变量和自变量之间的线性关系，并显示了因变量(y)如何根据自变量(x)而变化。

它试图在因变量和自变量之间建立一条最佳拟合线，这条最佳拟合线被称为回归线。

回归线的方程式为:

y= a<sub>0</sub>+ a*x+ b

这里，y=因变量

x=独立变量

a <sub>0</sub> =直线截距。

线性回归进一步分为两种类型:

*   **简单线性回归:**在简单线性回归中，单个自变量用于预测因变量的值。
*   **多元线性回归:**多元线性回归中，用多个自变量预测因变量的值。

下图为根据身高预测体重的线性回归:[阅读更多..](https://www.javatpoint.com/linear-regression-in-machine-learning)

![Machine Learning Algorithms](../Images/880277fe3de6d4d0bdb8218f213eadcc.png)

### 2.逻辑回归

逻辑回归是监督学习算法，用于**预测分类变量或离散值**。可用于机器学习中的*分类问题，逻辑回归算法的输出可以是是或否、0 或 1、红色或蓝色等。*

逻辑回归与线性回归相似，除了它们的使用方式，例如线性回归用于解决回归问题和预测连续值，而逻辑回归用于解决分类问题和预测离散值。

它不是拟合最佳拟合线，而是形成一条介于 0 和 1 之间的 S 形曲线。S 形曲线也被称为使用阈值概念的逻辑函数。任何高于阈值的值将趋向于 1，低于阈值的值将趋向于 0。[阅读更多..](https://www.javatpoint.com/logistic-regression-in-machine-learning)

### 3.决策树算法

决策树是一种监督学习算法，主要用于解决分类问题，但也可以用于解决回归问题。它可以处理分类变量和连续变量。它显示了一个树状结构，包括节点和分支，从根节点开始，扩展到其他分支，直到叶节点。**内部节点**用于表示数据集的**特征，分支表示决策规则，**和**叶节点表示问题的结果。**

决策树算法的一些现实应用是癌细胞和非癌细胞之间的识别，给顾客买车的建议等。[阅读更多..](https://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm)

### 4.支持向量机算法

支持向量机或 SVM 是一种监督学习算法，也可以用于分类和回归问题。但是，它主要用于分类问题。SVM 的目标是创建一个超平面或决策边界，可以将数据集分成不同的类。

帮助定义超平面的数据点被称为**支持向量**，因此被命名为支持向量机算法。

SVM 的一些现实应用有**人脸检测、图像分类、毒品发现**等。考虑下图:

![Machine Learning Algorithms](../Images/5890b32336d101211ec8d83825fe0c17.png)

如上图所示，超平面将数据集分为两个不同的类别。[阅读更多..](https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm)

### 5.天真的贝叶斯算法:

朴素贝叶斯分类器是一种监督学习算法，用于根据对象的概率进行预测。该算法被命名为朴素贝叶斯，因为它基于**贝叶斯定理**，并遵循*朴素*假设，即“变量相互独立”。

贝叶斯定理是基于条件概率的；它意味着当给出事件(B)已经发生时，事件(A)将发生的可能性。贝叶斯定理的公式如下:

![Machine Learning Algorithms](../Images/6eca0d1c27c6996fb0268cda4f21ef57.png)

朴素贝叶斯分类器是为给定问题提供良好结果的最佳分类器之一。建立一个简单的贝叶斯模型很容易，并且非常适合大量的数据集。多用于**文本分类**。[阅读更多..](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)

### 6.最近的邻居(KNN)

k-近邻是一种监督学习算法，可以用于分类和回归问题。该算法通过假设新数据点和可用数据点之间的相似性来工作。基于这些相似性，新的数据点被放在最相似的类别中。它也被称为懒惰学习算法，因为它存储所有可用的数据集，并在 K 邻居的帮助下对每个新案例进行分类。新案例被分配给最相似的最近的类，任何距离函数都测量数据点之间的距离。根据需要，距离函数可以是**欧几里德距离、闵可夫斯基距离、曼哈顿距离或海明距离**。[阅读更多..](https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning)

### 7.k-均值聚类

K-means 聚类是最简单的无监督学习算法之一，用于解决聚类问题。基于相似性和差异性将数据集分成 K 个不同的聚类，这意味着具有大部分共性的数据集保留在一个聚类中，而其他聚类之间的共性很少或没有。在 K-means 中，K 指的是聚类的数量，**指的是**指的是为了找到质心而对数据集进行平均。

这是一种基于质心的算法，每个聚类都与一个质心相关联。该算法旨在减少簇内数据点与其质心之间的距离。

该算法从一组随机选择的质心开始，这些质心在开始时形成簇，然后执行迭代过程来优化这些质心的位置。

它可以用于垃圾邮件检测和过滤，识别假新闻等。[阅读更多..](https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning)

### 8.随机森林算法

随机森林是一种有监督的学习算法，可用于机器学习中的分类和回归问题。它是一种集成学习技术，通过组合多个分类器来提供预测，并提高模型的性能。

*包含给定数据集子集的多棵决策树，求平均值提高模型的预测精度。随机森林应该包含 64-128 棵树。树的数量越多，算法的精度越高。*

为了对新的数据集或对象进行分类，每棵树给出分类结果，并且基于多数票，算法预测最终输出。

随机森林是一种快速算法，可以有效地处理缺失和错误的数据。[阅读更多..](https://www.javatpoint.com/machine-learning-random-forest-algorithm)

### 9.Apriori 算法

Apriori 算法是用于解决关联问题的无监督学习算法。它使用频繁项集来生成关联规则，并被设计用于包含事务的数据库。在这些关联规则的帮助下，它决定了两个对象之间的联系有多强或多弱。该算法使用广度优先搜索和哈希树来有效地计算项目集。

该算法迭代地从大数据集中寻找频繁项集。

apriori 算法是由阿格拉瓦尔和斯里坎特在 1994 年提出的。主要用于菜篮子分析，有助于了解可以一起购买的产品。它还可以用于医疗保健领域，以发现患者的药物反应。[阅读更多..](apriori-algorithm-in-machine-learning)

### 10.主成分分析

主成分分析是一种无监督学习技术，用于降维。它有助于降低包含许多相互关联的特征的数据集的维数。它是一个统计过程，借助正交变换将相关特征的观测值转换为一组线性不相关的特征。它是用于探索性数据分析和预测性建模的流行工具之一。

主成分分析通过考虑每个属性的方差来工作，因为高方差显示了类之间的良好分割，因此它降低了维度。

主成分分析的一些实际应用是图像处理、电影推荐系统、优化各种通信信道中的功率分配。[阅读更多..](principal-component-analysis)

* * ***