# 什么是纪元？

> 原文：<https://www.javatpoint.com/epoch>

机器学习中的一个时期意味着训练数据集完全通过算法。纪元数目是算法的一个重要超参数。它指定算法在训练或学习过程中经历的整个训练数据集的纪元或完整遍数。

对于每个时期，数据集的内部模型参数都会更新。因此，1 批次的历元称为批次梯度下降学习算法。通常，一个纪元的批次大小为 1 或更大，并且在纪元编号中始终是一个整数值。

它也可以被视为一个具有指定数量的纪元的“for-loop”，其中每个循环路径遍历整个训练数据集。for-loop 是一个嵌套的 for-loop，当“批次大小”数指定为 1 时，它允许循环迭代一个批次中的指定样本数。

当训练算法可以在数千个时期中运行并且该过程被设置为继续直到模型误差足够低时，时期数量的典型值。通常，教程和示例使用 10、500、100、1000 甚至更大的数字。

可以为训练过程创建线图，其中 x 轴是机器学习的历元，y 轴是技能或模型误差。这种类型的线图被称为算法的学习曲线，有助于诊断问题，例如根据需要向下、向上或向下学习训练集。

## 纪元和批次之间的差异

当处理特定数量的样本时，模型会更新，即样本的批次大小。训练数据集的完整遍数也很重要，被称为训练数据集中机器学习数的历元。批次大小通常等于 1，并且可以等于或小于训练数据集的样本数。神经网络中的历元或历元数通常是介于 1 和无穷大之间的整数值。因此，可以在任何时间段运行该算法。为了防止算法运行，可以在模型误差率随时间的变化中使用固定的历元数和因子。

在机器学习算法中，批量和时期都是超参数，包含整数作为训练模型使用的值。学习过程找不到这些值，因为它们不是模型的固有参数，并且在训练数据集上训练算法时必须为该过程指定。这些数字也不是固定值，根据算法的不同，可能需要尝试不同的整数值，然后才能找到最适合该过程的值。

## 例子

考虑这个来自机器学习时代的例子。假设一个数据集有 200 个样本(其中样本是指数据行)，有 1000 个纪元和 5 个批次大小来定义划时代。然后，数据集包含 40 个批次中每一个批次的 5 个样本，当每一批 5 个样本通过时，模型权重被更新。同样，在这种情况下，机器学习在一个纪元中由 40 个批次组成，这意味着模型将被更新 40 次。

此外，由于纪元计数为 1，000，整个数据集经过模型，而模型本身经过 1.000 次运行。当模型有 40 个批次或更新时，意味着在这个数据集上训练算法的过程中使用的训练数据集中有 40，000 个批次！

## 结论

在探索机器学习和批处理时代随机梯度下降的差异时，任何人都可以说梯度下降随机算法使用数据集进行训练，其学习算法在更新模型时迭代。

批次大小是一个梯度下降超参数，它在更新模型的内部参数以处理整个批次之前，测量要训练的训练样本的数量。同样，历元数是一个梯度下降超参数，它定义了通过训练数据集时的完整遍数。

* * *