# 学习和适应

> 原文:[https://www . javatpoint . com/人工神经网络学习与适应](https://www.javatpoint.com/artificial-neural-network-learning-and-adaption)

人工神经网络完全是受生物神经系统工作方式的启发。例如，人脑是工作的。人脑最强大的属性就是适应，ANN 也获得了类似的特性。我们应该明白我们的大脑到底是如何工作的？它仍然非常原始，尽管我们对程序有基本的了解。人们普遍认为，在学习过程中，大脑的神经结构会发生改变，从而增加或减少依赖其活动的突触连接能力。这就是为什么更相关的信息比长时间没有被审查的信息更容易审查的原因。更重要的信息会有强大的突触联系，而不太适用的信息会逐渐使其突触联系减弱，使其更难复习。

人工神经网络可以通过改变网络中神经元之间的加权关联来模拟这一学习过程。它有效地模拟了我们大脑中突触联系的加强和减弱。协会的加强和削弱使网络能够适应。人脸识别是一个人类很难精确转换成代码的例子。学习算法无法更好地解决的一个问题是贷款发放机构可以使用以前的信用评分来对未来的贷款概率进行分类。

学习规则是一种鼓励神经网络从现有条件中获益并提升其性能的技术或数学逻辑。这是一个反复的过程。在本教程中，我们将讨论神经网络中的学习规则。在这里，我们将讨论什么是 Hebbian 学习规则、感知学习规则、Delta 学习规则、相关性学习规则、out star 学习规则？所有这些神经网络学习规则将在下面用它们的数学公式详细讨论。

学习规则或学习过程是一种技术或数学逻辑。它提高了人工神经网络的性能，并在网络上实现了这一规则。因此，当网络模拟特定的数据环境时，学习规则会刷新网络的权重和偏差水平。

![Learning and Adaption](../Images/35b6f9decefec3565cb78242940bfa49.png)

## 希伯来人学习法则:

希伯来人的规则是主要的学习规则。1949 年，**唐纳德·赫布**创造了这种无监督神经网络的学习算法。我们可以使用这个规则来识别如何提高网络节点的权重。

Hebb 学习规则认为，如果相邻的神经元同时被激活和去激活，那么与这些神经元相关的权重应该增加。对于在相反阶段工作的神经元，它们之间的权重应该会减少。如果没有输入信号关系，权重不应改变。

如果两个节点的输入为正或负，则节点之间存在正权重。如果一个节点的输入对其他节点来说是正的或负的，则在节点之间存在一个实的负权重。

开始时，所有权重的值都设置为零。这个学习规则可以用于简单和硬激活功能。由于在学习过程中没有利用神经元的期望反应，这就是无监督学习规则。权重的绝对值与学习时间成正比，这是不希望的。

根据 Hebbian 学习规则，下面给出了在每个时间帧增加连接权重的公式。

**∵π(t)=δpi(t)* QJ(t)**

**这里，**

**∏ωij(t)=**权重的连接在时间函数 t 增加的增量。

α =恒定的正学习率。

**pi(t) =** 突触前神经元在时间 t 的函数下的输入值。

**qj(t) =** 突触前神经元的输出与时间 t 的函数相同。

我们知道，神经网络中的每个关联都有一个关联权重，这个权重会在整个学习过程中发生变化。它是监督学习的一个例子，网络通过给每个权重分配一个随机变量来开始学习。在这里，我们可以根据一组记录来评估输出值，对于这些记录，我们可以知道预测的输出值。**罗森布拉特**介绍了这条规则。这个学习规则是监督训练的一个例子，其中学习规则是用适当网络行为的一组例子给出的:

**{X1，t1}，{x2，t2}，…，{xq，tq}**

**在哪里，**

**Xq** =网络输入。

**tq** =目标输出。

当每个输入被提供给网络时，网络输出与网络目标进行比较。之后，学习规则改变权重并使网络偏向，以使网络输出更接近目标。

**单神经元感知器:**

在不同的计算机应用中，例如分类、模式识别和预测，学习模块可以通过不同的方法执行，包括结构、统计和神经方法。在这些技术中，人工神经网络的灵感来自大脑的生理操作。它们依赖于单个神经细胞(神经元)的科学模型，称为单神经元感知器，并试图类似于大脑中神经元的实际网络。

考虑一个具有一个神经元的双输入感知器，如下图所示。

![Learning and Adaption](../Images/9c219f82ad8341f7250ab47d2c3dc6d5.png)

**该网络的输出由**决定

**P = hard lim(n)= hard lim(W<sub>x</sub>+m)**

**= hard lim(1<sup>wT</sup>x+m)= hard lim(W<sub>1，1</sub> x <sub>1 +</sub> W <sub>1，2</sub> x <sub>2 + m</sub> )**

**多神经元感知器:**

对于具有多个神经元的感知器，单个神经元将有一个决策边界。神经元的决策边界将由下式定义

**<sub>I</sub>w<sup>T</sup>x+m<sub>I</sub><sub>= 0</sub>**

单神经元感知器可以将输入向量分为两类，因为它的输出可以是零或 1。多神经元感知器可以将输入分为许多类。不同的输出向量显示每个类。由于输出向量的每个分量可以是零或 1，因此总共有可能的**2<sup>S</sup>T3】个可能的类，其中 **s** 是神经元的数量。**

**数学方程:**

为了描述它的数学方程，假设我们有 n 个有限输入向量 x ^ n，以及它的期望输出向量 t ^ n，其中 n= 1 到 n。

如前所述，输出“k”可以基于净输入来确定，并且应用于该净输入的激活函数可以表示如下:

**K = f(K 中的<sub>)= 1，K>中的<sub>θ</sub>T5】</sub>**

**0，k <sub>in ≤ θ</sub>**

在哪里

θ =阈值。

对于这两种情况，可以确定不同的权重。

情况 1 -当 t ≠ k 时，则

**w(新)= w(旧)+ tx**

情况 2–当 t = k 时，则

**重量无变化**

## 增量学习规则:

人工神经网络中的 delta 规则是一种特定的反向传播，有助于改进机器学习/人工智能网络，使输入和输出与不同层的人工神经元相关联。增量规则也称为增量学习规则。

通常，反向传播与利用梯度技术重新计算人工神经元的输入权重有关。增量学习通过利用目标激活和获得的激活之间的差异来实现这一点。通过使用线性激活函数，网络连接得以平衡。另一种解释 Delta 规则的方法是，它使用误差函数来执行梯度下降学习。

Delta 规则是指将实际输出与目标输出进行比较，技术试图发现匹配，程序进行更改。增量规则的实际执行将根据网络及其组成而波动。然而，通过应用线性激活函数，delta 规则可以用于精炼具有特定类型反向传播的几类神经网络。

Delta 规则是由**维卓尔和霍夫**引入的，是依赖于监督学习的最有意义的学习规则。

该规则指出，节点权重的变化相当于误差和输入的乘积。

## 数学方程式:

给定的方程给出了 delta 学习规则的数学方程:

∆w =宽 x 深 z

* w = s(t-y)x

在这里，

**∮w**=重量变化。

=恒定的正学习率。

**X** =突触前神经元的输入值。

**z= (t-y)** 是期望输入 t 和实际输出 y 之间的差值。上述数学规则只能用于单个输出单元。

对于这两种情况，可以确定不同的权重。

情况 1 -当 t ≠ k 时，则

**w(新)= w(旧)+⇼w**

情况 2 -当 t = k 时，则

**重量无变化**

对于给定的输入向量，我们需要比较输出向量，最终的输出向量将是正确的答案。如果差异为零，则不会发生学习，因此我们需要调整权重来减少差异。如果输入模式的集合取自一个独立的集合，那么它使用增量学习规则来学习任意的连接。它已经检查了没有隐藏单元的具有线性激活函数的网络。误差平方与权重的关系图是 n 空间中的抛物面形状。比例常数是负的，所以这样一个函数的图形向上凹，数值最小。抛物面的顶点代表减小误差的点。权重向量是将这个点与理想的权重向量进行比较。我们可以利用单个输出单元和多个输出单元的增量学习规则。当我们应用增量学习规则来减少实际输出和可能输出之间的差异时，我们会发现一个错误。

## 相关学习规则:

相关学习规则基于与赫伯学习规则相同的原则。它认为相应神经元之间的权重应该是正的，具有逆反应的神经元之间的权重应该是渐进负的。与 Hebbian 规则相反，相关规则是监督学习。代替实际响应，oj(期望响应)，dj(用于重量计算)。

相关规则的数学公式如下:

√w<sub>ij =</sub>x<sub>I</sub>d<sub>j</sub>

训练算法通常从权重等于零的初始化开始。由于赋予用户期望的权重，相关学习规则是监督学习的一个例子。

在哪里

dj=输出信号的期望值。

## 外星学习规则:

在外星形学习规则中，需要与特定节点相关联的权重，并且它应该与与这些权重相关联的神经元的期望输出相同。这是受监督的训练过程，因为期望的输出必须是已知的。**格罗斯伯格**引入了分站学习规则。

√w<sub>ij =</sub>c(d<sub>I</sub>-w<sub>ij</sub>

在哪里

**di** 是期望的神经元输出。

**c** 为小学习常数，在学习过程中进一步减小。

* * *