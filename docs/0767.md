# 线性回归与逻辑回归

> 原文:[https://www . javatpoint . com/线性回归-vs-logistic-回归-机器学习](https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning)

线性回归和逻辑回归是监督学习技术下的两种著名的机器学习算法。由于这两种算法本质上都是有监督的，因此这些算法使用带标签的数据集来进行预测。但是它们之间的主要区别在于它们是如何被使用的。线性回归用于解决回归问题，而逻辑回归用于解决分类问题。下面给出了这两种算法的描述以及差异表。

![inear Regression vs Logistic Regression](../Images/421b5b7cfdb9bfe4909c1ec596a5e1d4.png)

## 线性回归:

*   线性回归是最简单的机器学习算法之一，属于监督学习技术，用于解决回归问题。
*   它用于借助自变量预测连续因变量。
*   线性回归的目标是找到能够准确预测连续因变量输出的最佳拟合线。
*   如果单个独立变量用于预测，那么它被称为简单线性回归，如果有两个以上的独立变量，那么这种回归被称为多元线性回归。
*   算法通过寻找最佳拟合线，建立因变量和自变量之间的关系。这种关系应该是线性的。
*   线性回归的输出应该只是价格、年龄、工资等连续值。因变量和自变量之间的关系如下图所示:

![inear Regression vs Logistic Regression](../Images/4eae03fac8fb1406dae812b255fe4bdc.png)

上图中因变量在 Y 轴(工资)，自变量在 x 轴(经验)。回归线可以写成:

```
y= a0+a1x+ ε

```

其中，a <sub>0</sub> 和 a <sub>1</sub> 为系数，ε为误差项。

## 逻辑回归:

*   逻辑回归是最流行的机器学习算法之一，属于监督学习技术。
*   它可以用于分类以及回归问题，但主要用于分类问题。
*   逻辑回归被用来在自变量的帮助下预测分类因变量。
*   逻辑回归问题的输出只能在 0 和 1 之间。
*   当需要两类之间的概率时，可以使用逻辑回归。比如今天会不会下雨，是 0 还是 1，是真是假等等。
*   逻辑回归是基于最大似然估计的概念。根据这种估计，观察到的数据应该是最有可能的。
*   在逻辑回归中，我们通过激活函数传递输入的加权和，激活函数可以映射 0 到 1 之间的值。这样的激活函数称为**乙状线函数**，得到的曲线称为乙状线或 S 曲线。请考虑下图:

![inear Regression vs Logistic Regression](../Images/358914bb0dce29dfe9fa7a147525b694.png)

*   逻辑回归方程为:

![inear Regression vs Logistic Regression](../Images/d2f531cd3aa23161e2d89ddc5ae74486.png)

线性回归和逻辑回归的区别:

| 线性回归 | 逻辑回归 |
| 线性回归用于使用一组给定的自变量预测连续因变量。 | 逻辑回归用于使用一组给定的自变量预测分类因变量。 |
| 线性回归用于解决回归问题。 | 逻辑回归用于解决分类问题。 |
| 在线性回归中，我们预测连续变量的值。 | 在逻辑回归中，我们预测分类变量的值。 |
| 在线性回归中，我们找到最佳拟合线，通过它我们可以很容易地预测输出。 | 在逻辑回归中，我们找到了 S 曲线，通过它我们可以对样本进行分类。 |
| 最小二乘估计方法用于估计精度。 | 最大似然估计方法用于估计精度。 |
| 线性回归的输出必须是连续值，如价格、年龄等。 | 逻辑回归的输出必须是分类值，如 0 或 1、是或否等。 |
| 在线性回归中，要求因变量和自变量之间的关系必须是线性的。 | 在逻辑回归中，不要求因变量和自变量之间具有线性关系。 |
| 在线性回归中，自变量之间可能存在共线性。 | 在逻辑回归中，自变量之间不应存在共线性。 |

* * *