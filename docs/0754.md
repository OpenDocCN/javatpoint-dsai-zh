# 机器学习中的线性回归

> 原文:[https://www . javatpoint . com/机器学习中的线性回归](https://www.javatpoint.com/linear-regression-in-machine-learning)

线性回归是最简单和最流行的机器学习算法之一。这是一种用于预测分析的统计方法。线性回归对连续/真实或数字变量进行预测，如**销售额、工资、年龄、产品价格、**等。

线性回归算法显示因变量(y)和一个或多个自变量(y)之间的线性关系，因此称为线性回归。因为线性回归显示线性关系，这意味着它发现因变量的值如何根据自变量的值而变化。

线性回归模型提供了代表变量之间关系的倾斜直线。请考虑下图:

![Linear Regression in Machine Learning](../Images/0fc26ac4fe88e2aa81547ce707d88c6d.png)

数学上，我们可以将线性回归表示为:

```
y= a0+a1x+ ε

```

**这里，**

Y=因变量(目标变量)
X=自变量(预测变量)
a0=直线截距(给出额外自由度)
a1 =线性回归系数(每个输入值的比例因子)。
ε =随机误差

x 和 y 变量的值是线性回归模型表示的训练数据集。

## 线性回归的类型

线性回归可以进一步分为两种算法:

*   **简单线性回归:**
    如果用单个自变量来预测数值因变量的值，那么这样的线性回归算法称为简单线性回归。
*   **多元线性回归:**
    如果用一个以上的自变量来预测一个数值因变量的值，那么这样的线性回归算法称为多元线性回归。

## 线性回归线

显示因变量和自变量之间关系的直线叫做**回归线**。回归线可以显示两种类型的关系:

*   **正线性关系:**
    如果因变量在 Y 轴增加，自变量在 X 轴增加，那么这样的关系称为正线性关系。

![Linear Regression in Machine Learning](../Images/b61f3fe4a012220ca4098a45ec73c2d7.png)

*   **负线性关系:**
    如果因变量在 Y 轴上减少，自变量在 X 轴上增加，那么这样的关系称为负线性关系。

![Linear Regression in Machine Learning](../Images/a0f776c9c986d9b0d428ca66403a0e82.png)

## 找到最佳拟合线:

当使用线性回归时，我们的主要目标是找到最佳拟合线，这意味着预测值和实际值之间的误差应该最小化。最佳拟合线的误差最小。

线的权重或系数的不同值(a <sub>0</sub> 、a <sub>1</sub> )给出了不同的回归线，因此我们需要计算 a <sub>0</sub> 和 a <sub>1</sub> 的最佳值以找到最佳拟合线，因此为了计算这一点，我们使用成本函数。

### 成本函数-

*   线的权重或系数的不同值(a <sub>0</sub> 、a <sub>1</sub> )给出了不同的回归线，并且成本函数用于估计最佳拟合线的系数值。
*   成本函数优化回归系数或权重。它衡量线性回归模型的表现。
*   我们可以用代价函数来求**映射函数**的精度，它把输入变量映射到输出变量。这个映射函数也被称为**假设函数**。

对于线性回归，我们使用**均方误差(MSE)** 成本函数，它是预测值和实际值之间出现的均方误差的平均值。它可以写成:

对于上述线性方程，均方误差可以计算为:

![Linear Regression in Machine Learning](../Images/4d7c7e4d56cc2d5422ab4da5735ff356.png)

**在哪里，**

n =观测总次数
Yi =实际值
(a1x I+a 0)=预测值。

**残差:**实际值和预测值之间的距离称为残差。如果观察点离回归线很远，那么残差会很高，成本函数也会很高。如果分散点靠近回归线，那么残差将很小，因此成本函数也很小。

### 梯度下降:

*   梯度下降用于通过计算代价函数的梯度来最小化均方误差。
*   回归模型使用梯度下降通过减少成本函数来更新线的系数。
*   这是通过随机选择系数值，然后迭代地更新数值以达到最小成本函数来实现的。

## 模型性能:

拟合优度决定了回归线如何拟合观测值集。从各种模型中找出最佳模型的过程称为**优化**。这可以通过以下方法实现:

**1。r 平方法:**

*   r 平方是一种确定拟合优度的统计方法。
*   它在 0-100%的范围内测量因变量和自变量之间关系的强度。
*   R 平方的高值决定了预测值和实际值之间的较小差异，因此代表了一个良好的模型。
*   也称为多重回归的**决定系数、**或**多重决定系数**。
*   可以通过以下公式计算:

![Linear Regression in Machine Learning](../Images/6b907bc03b1a6951f2979202476da87c.png)

## 线性回归的假设

以下是线性回归的一些重要假设。这些是构建线性回归模型时的一些正式检查，确保从给定的数据集获得最佳结果。

*   **特征和目标之间的线性关系:**
    线性回归假设因变量和自变量之间的线性关系。
*   **特征之间的多重共线性很小或没有:**
    多重共线性意味着自变量之间的高度相关性。由于多重共线性，可能很难找到预测因子和目标变量之间的真实关系。或者我们可以说，很难确定哪个预测变量在影响目标变量，哪个没有。因此，该模型假设特征或自变量之间很少或没有多重共线性。
*   **同质性假设:**
    同质性是指误差项对自变量的所有值都相同的情况。由于同质性，散点图中的数据应该没有明确的模式分布。
*   **误差项的正态分布:**
    线性回归假设误差项应遵循正态分布模式。如果误差项不是正态分布，那么置信区间将变得太宽或太窄，这可能导致难以找到系数。
    可以使用 **q-q 图**进行检查。如果图中显示直线没有任何偏差，这意味着误差是正态分布的。
*   **无自相关:**
    线性回归模型假设误差项无自相关。如果误差项中存在任何相关性，那么它将大大降低模型的精度。如果残差之间存在相关性，通常会出现自相关。

* * *