# 美国有线电视新闻网的 MNIST 数据集

> 原文:[https://www.javatpoint.com/tensorflow-mnist-dataset-in-cnn](https://www.javatpoint.com/tensorflow-mnist-dataset-in-cnn)

MNIST ( **改良国家标准与技术研究所**)数据库是一个手写数字或数字的大型数据库，用于训练各种图像处理系统。该数据集还广泛用于**机器学习**领域的训练和测试。MNIST 数据库中的这组图像是 NIST 的两个数据库的组合:特殊数据库 1 和特殊数据库 3。

MNIST 数据集有 **6 万张**训练图像和 **1 万张**测试图像。

**MNIST** 数据集可以在线，本质上是各种手写数字的数据库。MNIST 数据集拥有大量数据，通常用于展示深度神经网络的真正能力。我们的大脑和眼睛一起工作来识别任何编号的图像。我们的大脑是一个强大的工具，它能够快速地对任何图像进行分类。一个数字的形状有这么多，我们的大脑可以很容易地识别这些形状，并确定它是什么数字，但同样的任务对计算机来说并不简单。只有一种方法可以做到这一点，那就是使用深度神经网络，它允许我们训练一台计算机来有效地对手写数字进行分类。

![MNIST Dataset in CNN](../Images/d9e202c90313995a1659e167f426c8c1.png)

因此，我们只处理了包含在**笛卡尔坐标系**上的简单数据点的数据。从开始到现在，我们已经分发了二进制类数据集。当我们使用多类数据集时，我们将使用**软最大激活**功能，这对于二进制数据集的分类非常有用。在 0 和 1 之间排列值非常有效。sigmoid 函数对于多因果数据集无效，为此，我们使用 softmax 激活函数，它能够处理它。

MNIST 数据集是一个多级数据集，由 10 个类组成，我们可以在其中对 0 到 9 的数字进行分类。我们以前使用的数据集和 MNIST 数据集之间的主要区别在于 MNIST 数据输入神经网络的方法。

在感知模型和线性回归模型中，每个数据点由简单的 x 和 y 坐标定义。这意味着输入层需要两个节点来输入单个数据点。

在 MNIST 数据集中，单个数据点以图像的形式出现。包含在 **MNIST** 数据集中的这些图像是典型的 **28*28** 像素，例如 28 个像素穿过横轴，28 个像素穿过纵轴。这意味着来自 MNIST 数据库的单个图像总共有 784 个像素需要分析。我们的神经网络的输入层有 784 个节点来解释这些图像之一。

![MNIST Dataset in CNN](../Images/20984361887ba55af3cee654ca3c1393.png)

在这里，我们将看到如何创建一个函数，该函数是通过查看图像中的每个像素来识别手写数字的模型。然后使用张量流来训练模型，通过让它看成千上万个已经标记的例子来预测图像。然后，我们将使用测试数据集检查模型的准确性。

**TensorFlow 中的 MNIST 数据集，包含手写数字拆分成三部分的信息:**

*   **训练数据(mnist.train) -55000 个数据点**
*   **验证数据(mnist.validate) -5000 数据点**
*   **测试数据(mnist.test) -10000 个数据点**

现在，在我们开始之前，需要注意的是，每个数据点都有两个部分:一个图像(x)和描述实际图像的相应标签(y)，每个图像都是一个 28x28 数组，即 784 个数字。图像的标签是 0 到 9 之间的数字，对应于张量流 MNIST 图像。要下载和使用 MNIST 数据集，请使用以下**命令**:

```

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

```

![MNIST Dataset in CNN](../Images/7a9d258f788454a0c1e2d3086ed51b79.png)

## 张量流中的软最大回归

张量流 MNIST 从 0 到 9 只有十种可能性。我们的目标是看着一幅图像，以特定的概率说给定的图像是一个特定的数字。当回归给我们的值介于 0 和 1 之间且总和为 1 时，有可能使用 Softmax。因此，我们的方法应该很简单。

![MNIST Dataset in CNN](../Images/d6da722edc007d392fcbaa8cf70cf6a7.png)
![MNIST Dataset in CNN](../Images/f7f41197c5d67d74afe9daa49f9f97c1.png)

我们将张量流 MNIST 图像分类为特定类别，然后将其表示为正确与否的概率。现在，完全取决于特定类别中的所有对象，我们可以对像素强度进行加权求和。我们还需要添加一个偏差，以表明某些事情更有可能独立于输入。Softmax 对权重进行归一化，并将其相加，假设权重为负或零。

## 张量流中 MNIST 数据集的实现

使用 TensorFlow MNIST 数据集分类的好处是，它让我们可以描述完全在 Python 之外运行的交互操作的图形。

首先，我们使用导入张量流库

```

Import tensorflow as tf

```

然后我们创建一个占位符，当我们要求库运行计算时，我们将输入这个值

```

x = tf.placeholder (tf.float32, [None, 784])

```

然后我们应该给我们的模型增加权重和偏差。使用变量，变量是一个可修改的张量，在交互操作图中有一个范围。

```

W= tf.Variable (tf.zeros([784,10]))
b= tf.Variable(tf.zeros([10]))

```

请注意，W 的形状是[784，10]，因为我们希望通过将 784 维图像向量乘以它来产生不同类别的 10 维证据向量。我们可以将 b 添加到输出中，因为它的形状为[10]。

## TensorFlow MNIST-培训

我们通过将特征矩阵与权重相乘来定义模型，并向其添加偏差，然后通过 softmax 函数运行它。

```

y = tf.nn.softmax(tf.matmul(x,W) +b)

```

我们使用成本函数或均方误差函数来找出我们的结果与实际数据的偏差。误差越小，模型越好。另一个非常常见的函数是交叉熵，它衡量我们的预测有多低效。该函数描述如下，其中 y 代表预测值，y '是实际分布。我们通过添加一个占位符来实现它。

```

y_ =tf.placeholder(tf.float32, [None, 10])

```

然后定义交叉熵

```

cross_entropy= tf.reduce_mean(-tf.reduce_sum(y_ *tf.log(y), reduction_indices=[1]))

```

既然我们已经成功定义了我们的模型，是时候训练它了。我们可以在梯度下降和反向传播的帮助下做到这一点。还有许多其他可用的优化算法，如逻辑回归、动态松弛等。我们可以使用学习率为 0.5 的梯度下降进行代价函数优化。

```

train_step= tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

```

在训练之前，我们需要启动一个会话，并初始化我们之前创建的变量。

```

sess= tf.InteractiveSession()

```

这将启动一个交互式会话，变量由初始化

```

tf.global_variables_initializer().run()

```

现在，我们必须训练网络。我们应该改变时代的数量来适应我们的模式。

```

for _in range (1000):
batch_xs, batch_ys =mnist.train.next_batch(100)
sess.run(train_step, feed_dict={x:batch_xs, y_:batch_ys})

```

### 使用测试数据集检查准确性

我们通过将我们的结果与测试数据集进行比较来检查准确性。这里，我们可以利用 tf.argmax 函数，它让我们知道沿着特定轴的张量中最高值的索引。

```

correct_prediction = tf.equal (tf.argmax (y, 1),
tf.argmax(y_,1))

```

这给了我们布尔人的列表，然后我们在转换成浮点数后取平均值。

```

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

```

然后，我们可以通过以下方式打印出精度

```

print (sess.run (accuracy, feed_dict= {x: mnist.test.images, y_:mnist.test.labels}))

```

* * *