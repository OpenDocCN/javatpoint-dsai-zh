# 主成分分析

> 原文:[https://www.javatpoint.com/principal-component-analysis](https://www.javatpoint.com/principal-component-analysis)

主成分分析是一种无监督学习算法，用于[机器学习](https://www.javatpoint.com/machine-learning)中的降维。它是一个统计过程，借助正交变换将相关特征的观测值转换为一组线性不相关的特征。这些新的转换特征被称为**主成分**。它是用于探索性数据分析和预测性建模的流行工具之一。这是一种通过减少方差从给定数据集中提取强模式的技术。

主成分分析通常试图找到低维表面来投影高维数据。

主成分分析通过考虑每个属性的方差来工作，因为高属性显示了类之间的良好分割，因此它降低了维度。PCA 的一些现实应用是 ***图像处理，电影推荐系统，优化各种通信渠道的功率分配。*** 它是一种特征提取技术，所以包含了重要变量，去掉了最不重要的变量。

主成分分析算法基于一些数学概念，例如:

*   方差和协方差
*   特征值和特征因子

主成分分析算法中使用的一些常用术语:

*   **维数:**是给定数据集中存在的特征或变量的数量。更容易的是数据集中的列数。
*   **相关性:**表示两个变量之间的关联有多强。例如，如果一个变量改变，另一个变量也会改变。相关值的范围从-1 到+1。这里，-1 表示变量之间成反比，+1 表示变量之间成正比。
*   **正交:**定义变量之间不相关，因此变量对之间的相关性为零。
*   **特征向量:**如果有一个方阵 M，并且给出一个非零向量 v。如果 Av 是 v 的标量倍数，v 就是特征向量。
*   **协方差矩阵:**包含一对变量之间协方差的矩阵称为协方差矩阵。

### 主成分分析中的主成分

如上所述，变换后的新特征或主成分分析的输出是主成分。这些点的数量等于或小于数据集中的原始要素。这些主要成分的一些性质如下:

*   主成分必须是原始特征的线性组合。
*   这些分量是正交的，即一对变量之间的相关性为零。
*   每个组件的重要性在去 1 到 n 的时候会降低，这意味着 1 个 PC 最重要，n 个 PC 将最不重要。

### 主成分分析算法步骤

1.  **获取数据集**
    首先，我们需要将输入数据集分为 X 和 Y 两个子集，其中 X 为训练集，Y 为验证集。
2.  **将数据表示成结构**
    现在我们将数据集表示成结构。比如我们将表示独立变量 x 的二维矩阵，这里每行对应数据项，列对应特征。列数是数据集的维度。
3.  **标准化数据**
    在这一步，我们将标准化我们的数据集。例如在特定列中，与方差较低的特征相比，方差较高的特征更重要。
    如果特征的重要性与特征的方差无关，那么我们就用列的标准差将每个数据项划分到一列中。这里我们将矩阵命名为 z。
4.  **计算 Z 的协方差**
    计算 Z 的协方差，我们取矩阵 Z，进行转置。转置后，我们将它乘以 z。输出矩阵将是 z 的协方差矩阵。
5.  **计算特征值和特征向量**
    现在我们需要计算结果协方差矩阵 z 的特征值和特征向量。特征向量或协方差矩阵是具有高信息的轴的方向。这些特征向量的系数被定义为特征值。
6.  **对本征向量**
    进行排序在这一步中，我们将获取所有的本征值，并将它们按照从大到小的递减顺序进行排序。同时对特征值矩阵 P 中的特征向量进行相应排序。结果矩阵将被命名为 P*。
7.  **计算新特征或主成分**
    这里我们将计算新特征。为此，我们将把 P*矩阵乘以 Z。在结果矩阵 Z*中，每个观察都是原始特征的线性组合。Z*矩阵的每一列相互独立。
8.  **从新数据集中移除较少或不重要的要素。**
    新的功能集已经出现，所以我们将在这里决定保留什么和删除什么。这意味着，我们将只保留新数据集中相关或重要的特征，不重要的特征将被移除。

## 主成分分析的应用

*   主成分分析主要用作计算机视觉、图像压缩等各种人工智能应用中的降维技术。
*   如果数据具有高维度，它还可以用于查找隐藏模式。使用主成分分析的领域有金融、数据挖掘、心理学等。

* * *