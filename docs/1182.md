# 深层神经网络中的反向传播过程

> 原文：<https://www.javatpoint.com/pytorch-backpropagation-process-in-deep-neural-network>

**反向传播**是神经网络的重要概念之一。我们的任务是对我们的数据进行最佳分类。为此，我们必须更新参数和偏差的权重，但是如何在深度神经网络中做到这一点呢？在线性回归模型中，我们使用梯度下降来优化参数。类似地，这里我们也使用梯度下降算法使用反向传播。

对于单个训练示例，**反向传播**算法计算**误差函数**的梯度。反向传播可以写成神经网络的函数。反向传播算法是一组用于有效训练人工神经网络的方法，遵循利用链式规则的梯度下降方法。

反向传播的主要特征是迭代、递归和有效的方法，通过这种方法，它计算更新的权重以改进网络，直到它不能执行它被训练的任务。反向传播需要在网络设计时已知激活函数的导数。

现在，误差函数是如何在反向传播中使用的，反向传播是如何工作的？让我们从一个例子开始，用数学方法来理解如何使用反向传播准确地更新权重。

![Backpropagation Process in Deep Neural Network](img/012a655245c021d73b2d56c35bc18c8b.png)

### 输入值

X1=0.05
X2=0.10

### 初重

w1 = 0.15 w5 = 0.40
W2 = 0.20 w6 = 0.45
W3 = 0.25 w7 = 0.50
W4 = 0.30 w8 = 0.55

### 偏差值

b1=0.35 b2=0.60

### 目标值

T1=0.01
T2=0.99

现在，我们首先通过向前传递计算 H1 和 H2 的值。

### 前进传球

为了找到 H1 的值，我们首先将权重的输入值乘以

H1 = x1w<sub>1</sub>+x2w<sub>2</sub>+B1
H1 = 0 . 050 . 15+0 . 100 . 20+0.35
+T6】H1 = 0.3775

为了计算 H1 的最终结果，我们执行 sigmoid 函数如下

![Backpropagation Process in Deep Neural Network](img/d88fda000ac71262b254cf3d132fac07.png)

我们将像计算 H1 一样计算 H2 的价值

H2 = x1w<sub>3</sub>+x2w<sub>4</sub>+B1
H2 = 0 . 050 . 25+0 . 100 . 30+0.35
+T6】H2 = 0.3925

为了计算 H1 的最终结果，我们执行 sigmoid 函数如下

![Backpropagation Process in Deep Neural Network](img/405d36b0164592dfd808e5a888790fca.png)

现在，我们计算 y1 和 y2 的值，就像计算 H1 和 H2 一样。

要找到 y1 的值，我们首先将输入值乘以权重，即 H1 和 H2 的结果，如下所示

y1 = H1w<sub>5</sub>+H2w<sub>6</sub>+B2
y1 = 0.5932699920.40+0.5968843780.45+0.60
+T6】y1 = 1.10590597

为了计算 y1 的最终结果，我们执行 sigmoid 函数如下

![Backpropagation Process in Deep Neural Network](img/95feeb28c578929c3dacefadf624d9b5.png)

我们将以与 y1 相同的方式计算 y2 的值

y2 = H1w<sub>7</sub>+H2w<sub>8</sub>+B2
y2 = 0.5932699920.50+0.5968843780.55+0.60
+T6】y2 = 1.2249214

为了计算 H1 的最终结果，我们执行 sigmoid 函数如下

![Backpropagation Process in Deep Neural Network](img/c1f2624ef245e491464f24a5df05ddd7.png)

我们的目标值是 0.01 和 0.99。我们的 y1 和 y2 值与我们的目标值 T1 和 T2 不匹配。

现在，我们将找到**总误差**，它只是输出与目标输出之间的差值。总误差计算如下

![Backpropagation Process in Deep Neural Network](img/77b4b3c27df02dc4982fe27564eecd5f.png)

总误差是

![Backpropagation Process in Deep Neural Network](img/556a2e51f3091d14ba52fb59ba803abe.png)

现在，我们将反向传播这个错误，使用反向传递来更新权重。

### 输出层的反向传递

为了更新权重，我们在总误差的帮助下计算每个权重对应的误差。重量 w 的误差是通过将总误差与 w 进行微分来计算的。

![Backpropagation Process in Deep Neural Network](img/2032a210de7cbf1aedc903e74c43f88b.png)

我们执行反向过程，因此首先考虑最后一个权重 w5

![Backpropagation Process in Deep Neural Network](img/3bffb18fc49fd7012f93d26d0701caf0.png)

从方程二，很明显，我们不能部分地区分它和 w5，因为没有任何 w5。我们把方程一分解成多个项，这样我们就可以很容易地把它和 w5 区分为

![Backpropagation Process in Deep Neural Network](img/c5e28ccf8402ab7015c6ddc1758f3fc3.png)

现在，我们逐一计算每一项，将 E <sub>总计</sub>与 w5 区分为

![Backpropagation Process in Deep Neural Network](img/28182da23b7dd208ae30f0d41e371e1d.png)

将 e <sup>-y</sup> 的值放入等式(5)

![Backpropagation Process in Deep Neural Network](img/5ad5c5f78f39370cfd4ad6252e7cf648.png)

因此，我们将![Backpropagation Process in Deep Neural Network](img/b73edee30ac486bd604b7af015aef1b9.png)的值放在等式(3)中，以找到最终结果。

![Backpropagation Process in Deep Neural Network](img/907fda2d500f8905458d6dda9e03d996.png)

现在，我们将借助以下公式计算更新后的权重 w5 <sub>新的</sub>

![Backpropagation Process in Deep Neural Network](img/348f75102cf68723e5d8c66f9a8fa8c0.png)

同样，我们计算 w6 <sub>新</sub>，w7 <sub>新</sub>，以及 w8 <sub>新</sub>，这将给出以下值

**w5 <sub>新增</sub>= 0.35891648**T4**w6<sub>新增</sub>= 408666186**T9**w7<sub>新增</sub>= 0.511301270**
T15】w8<sub>新增</sub> =0.561370121

### 隐藏层的反向通过

现在，我们将反向传播到我们的隐藏层，并更新权重 w1、w2、w3 和 w4，就像我们对 w5、w6、w7 和 w8 权重所做的那样。

我们将 w1 处的误差计算为

![Backpropagation Process in Deep Neural Network](img/faf6920ed0bd9f612bf9ac049fd4bd69.png)

从等式(2)中可以清楚地看出，我们不能部分区分它与 w1 的关系，因为没有任何 w1。我们将等式(1)分解为多个项，这样我们就可以很容易地将其与 w1 区分为

![Backpropagation Process in Deep Neural Network](img/fc6caa09cb1c9d2bdc4c060e89062b54.png)

现在，我们逐一计算每一项，将 E <sub>总计</sub>相对于 w1 微分为

![Backpropagation Process in Deep Neural Network](img/16953adce649c9c060ae2eac3f4042d4.png)

我们再一次把这个分开，因为在 E <sup>中没有任何 H1 <sup>最终</sup>术语，总的来说</sup>为

![Backpropagation Process in Deep Neural Network](img/fccb61e7b5ab1d7471a186089f59443f.png)

![Backpropagation Process in Deep Neural Network](img/57424c6f8cb3fc7cf8c0b07de0458705.png)将再次分裂，因为在 E1 和 E2 中没有 H1 项。拆分按如下方式进行

![Backpropagation Process in Deep Neural Network](img/57891e5e0b1639d017a4d3fb4935dc1b.png)

我们再次分割两个![Backpropagation Process in Deep Neural Network](img/46c2c927d8360eb71881d6a79cd3a274.png)，因为在 E1 和 E2 中没有任何 y1 和 y2 项。我们把它分成

![Backpropagation Process in Deep Neural Network](img/e5709a92e995cfb68994db4131094bea.png)

现在，我们通过将等式(18)和(19)中的值表示为

根据等式(18)

![Backpropagation Process in Deep Neural Network](img/c8a3e83e9c5aa7b31ca1b2fe6c897ede.png)

根据等式(8)

![Backpropagation Process in Deep Neural Network](img/09d529384e0337a3e020fa3bf51c5b8a.png)

根据等式(19)

![Backpropagation Process in Deep Neural Network](img/ed7033ea6d9d83a4109780f78d540ce5.png)

将 e <sup>-y2</sup> 的值代入等式(23)

![Backpropagation Process in Deep Neural Network](img/7225034fddf73efd102510e70ea6d2e7.png)

根据等式(21)

![Backpropagation Process in Deep Neural Network](img/4d25195ffcc864dd1de988836a24f6d5.png)

现在从等式(16)和(17)

![Backpropagation Process in Deep Neural Network](img/82dbbf7951bb61df81442490069472b4.png)

将等式(15)中![Backpropagation Process in Deep Neural Network](img/415f983738bf378d0bde9c6f0d1ee514.png)的值表示为

![Backpropagation Process in Deep Neural Network](img/97b8c63040e59f9539c160a9ceb45f40.png)

我们有![Backpropagation Process in Deep Neural Network](img/6cd992ee7cf1877a7c86abbee6c48525.png)我们需要算出![Backpropagation Process in Deep Neural Network](img/bda4e8659c9203a92e1cfe51bdb78c18.png)为

![Backpropagation Process in Deep Neural Network](img/6bc2cfe10d5cbeced2dc7cb2f49f48e3.png)

将 e <sup>-H1</sup> 的值放入等式(30)

![Backpropagation Process in Deep Neural Network](img/e002beb3530fa1b4fc5830f6007b10d6.png)

我们计算 H1 总净输入相对于 w1 的偏导数，与计算输出神经元的方法相同:

![Backpropagation Process in Deep Neural Network](img/42558b7769ec775f443150b36403594a.png)

因此，我们将![Backpropagation Process in Deep Neural Network](img/6a50cd55e102629f5b666ecd5292c01e.png)的值放在等式(13)中，以找到最终结果。

![Backpropagation Process in Deep Neural Network](img/a492ee63eb7937dfbb78c746d0bd77f3.png)

现在，我们将借助以下公式计算更新后的权重 w1 <sub>新的</sub>

![Backpropagation Process in Deep Neural Network](img/82ee0ebcfd1087962a9b13520231bd4e.png)

同样，我们计算 w2 <sub>新</sub>，w3 <sub>新</sub>，和 w4，这将给出以下值

**w1 <sub>新增</sub>= 0.149780716**T4**w2<sub>新增</sub>= 0.19956143**T9**w3<sub>新增</sub>= 0.24975114**
T15】w4<sub>新增</sub> =0.29950229

我们已经更新了所有的重量。当我们前馈 0.05 和 0.1 输入时，我们在网络上发现了错误 0.298371109。在第一轮反向传播中，总误差下降到 0.291027920005 重复此过程 10，000 次后，总误差降至 0.0000351085。此时，输出神经元生成 0.159121960 和 0.984065734，即在我们前馈 0.05 和 0.1 时的目标值附近。

* * *