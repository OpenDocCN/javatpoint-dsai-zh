# 支持向量机算法

> 原文:[https://www . javatpoint . com/机器学习-支持向量机-算法](https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm)

支持向量机或 SVM 是最流行的监督学习算法之一，用于分类和回归问题。然而，它主要用于机器学习中的分类问题。

SVM 算法的目标是创建能够将 n 维空间分成类的最佳线或决策边界，以便我们将来可以轻松地将新数据点放入正确的类别中。这个最佳决策边界称为超平面。

SVM 选择了有助于创建超平面的极值点/向量。这些极端情况称为支持向量，因此算法称为支持向量机。考虑下图，其中有两个不同的类别，使用决策边界或超平面进行分类:

![Support Vector Machine Algorithm](../Images/3f5067c7f65c2d87ba21e15d7e256799.png)

**例子:** SVM 可以用我们在 KNN 分类器中使用的例子来理解。假设我们看到一只奇怪的猫，它也有狗的一些特征，那么如果我们想要一个能够准确识别它是猫还是狗的模型，那么这样的模型可以通过使用 SVM 算法来创建。我们将首先用大量的猫和狗的图像来训练我们的模型，这样它就可以了解猫和狗的不同特征，然后我们用这个奇怪的生物来测试它。所以当支持向量在这两个数据(猫和狗)之间建立决策边界，选择极端情况(支持向量)时，就会看到猫和狗的极端情况。根据支持向量，它将把它归类为猫。考虑下图:

![Support Vector Machine Algorithm](../Images/edecd556de53d09ca378ad1601017af8.png)

SVM 算法可用于**人脸检测、图像分类、文本分类、**等。

## SVM 的类型

**SVM 可以有两种类型:**

*   **线性 SVM:** 线性 SVM 用于线性可分数据，这意味着如果一个数据集可以用一条直线分为两类，那么这样的数据称为线性可分数据，而使用的分类器称为线性 SVM 分类器。
*   **非线性 SVM:** 非线性 SVM 用于非线性分离的数据，这意味着如果数据集不能用直线分类，那么这种数据被称为非线性数据，使用的分类器被称为非线性 SVM 分类器。

## SVM 算法中的超平面和支持向量；

**超平面:**可以有多条线/决策边界来隔离 n 维空间中的类，但我们需要找出有助于数据点分类的最佳决策边界。这个最佳边界被称为 SVM 超平面。

超平面的维度取决于数据集中存在的特征，这意味着如果有 2 个特征(如图所示)，那么超平面将是一条直线。如果有 3 个特征，那么超平面就是一个二维平面。

我们总是创建一个具有最大余量的超平面，这意味着数据点之间的最大距离。

**支持向量:**

最接近超平面并且影响超平面位置的数据点或向量称为支持向量。由于这些向量支持超平面，因此称为支持向量。

## SVM 是如何运作的？

**线性 SVM:**

通过一个例子可以理解 SVM 算法的工作原理。假设我们有一个数据集，它有两个标签(绿色和蓝色)，数据集有两个要素 x1 和 x2。我们需要一个分类器，可以将坐标对(x1，x2)分类为绿色或蓝色。请考虑下图:

![Support Vector Machine Algorithm](../Images/5cd6bdcc338d2e3ccea9724b11e2c456.png)

因为它是二维空间，所以只要用一条直线，我们就可以很容易地把这两类分开。但是可以有多行来分隔这些类。请考虑下图:

![Support Vector Machine Algorithm](../Images/614eba894d8c6fae423fb3e11f61d7a2.png)

因此，SVM 算法有助于找到最佳线或决策边界；这个最佳边界或区域被称为**超平面**。SVM 算法从两个类中找到直线的最近点。这些点称为支持向量。向量和超平面之间的距离称为**边界**。SVM 的目标是最大化这个利润。具有最大余量的**超平面**被称为**最优超平面**。

![Support Vector Machine Algorithm](../Images/76fbe2d713bd6bcd881de973adcaba78.png)

**非线性 SVM:**

如果数据是线性排列的，那么我们可以用一条直线把它分开，但是对于非线性数据，我们不能画一条直线。请考虑下图:

![Support Vector Machine Algorithm](../Images/94f217ed760141f436f7f0a2bee163c2.png)

所以为了分离这些数据点，我们需要增加一个维度。对于线性数据，我们使用了两个维度 x 和 y，因此对于非线性数据，我们将添加第三个维度 z。它可以计算为:

```
z=x2 +y2

```

通过添加第三维，样本空间将变成如下图所示:

![Support Vector Machine Algorithm](../Images/228c0eab5a6e37fba3cd3ecaf1a56f35.png)

所以现在，SVM 将以下面的方式将数据集分成类。请考虑下图:

![Support Vector Machine Algorithm](../Images/cd6a4c3d1cc42353df752a2b72d30730.png)

因为我们在三维空间，所以它看起来像一个平行于 x 轴的平面。如果我们在 z=1 的 2d 空间中转换它，那么它将变成:

![Support Vector Machine Algorithm](../Images/5ff9671a74d978296f8ebc0f7b6b47a6.png)

因此，在非线性数据的情况下，我们得到半径为 1 的周长。

**支持向量机的 Python 实现**

现在我们将使用 Python 实现 SVM 算法。这里我们将使用相同的数据集 **user_data** ，我们已经在 Logistic 回归和 KNN 分类中使用了该数据集。

*   **数据预处理步骤**

在数据预处理步骤之前，代码将保持不变。下面是代码:

```

#Data Pre-processing Step
# importing libraries
import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd

#importing datasets
data_set= pd.read_csv('user_data.csv')

#Extracting Independent and dependent Variable
x= data_set.iloc[:, [2,3]].values
y= data_set.iloc[:, 4].values

# Splitting the dataset into training and test set.
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)
#feature Scaling
from sklearn.preprocessing import StandardScaler  
st_x= StandardScaler()  
x_train= st_x.fit_transform(x_train)  
x_test= st_x.transform(x_test)     

```

执行上述代码后，我们将对数据进行预处理。代码将数据集表示为:

![Support Vector Machine Algorithm](../Images/993ec3d85552b10cd7ecbed96a7e0de1.png)

测试集的缩放输出将是:

![Support Vector Machine Algorithm](../Images/9c0aa0442a80c7e2b1daaf0b1b14fc97.png)

**将 SVM 分类器拟合到训练集:**

现在训练集将适合 SVM 分类器。为了创建 SVM 分类器，我们将从 **Sklearn.svm** 库中导入 **SVC** 类。下面是它的代码:

```

from sklearn.svm import SVC # "Support vector classifier"
classifier = SVC(kernel='linear', random_state=0)
classifier.fit(x_train, y_train)

```

在上面的代码中，我们使用了**内核='linear'** ，因为这里我们为线性可分离的数据创建了 SVM。但是，我们可以针对非线性数据进行更改。然后我们将分类器拟合到训练数据集(x_train，y_train)

**输出:**

```
Out[8]: 
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
    kernel='linear', max_iter=-1, probability=False, random_state=0,
    shrinking=True, tol=0.001, verbose=False)

```

通过改变 **C(正则化因子)、γ和核**的值，可以改变模型性能。

*   **预测测试集结果:**
    现在，我们将预测测试集的输出。为此，我们将创建一个新的向量 y_pred。下面是它的代码:

```

#Predicting the test set result
y_pred= classifier.predict(x_test)

```

得到 y_pred 向量后，我们可以比较 **y_pred** 和 **y_test** 的结果，检查实际值和预测值的差异。

**输出:**以下是测试集预测的输出:

![Support Vector Machine Algorithm](../Images/f6182ee5a57c88bc6c5911c3cc08d7df.png)

*   **创建混淆矩阵:**
    现在我们将看到 SVM 分类器的性能，即与逻辑回归分类器相比，有多少不正确的预测。要创建混淆矩阵，我们需要导入 sklearn 库的**混淆矩阵**功能。导入函数后，我们将使用新变量 **cm** 调用它。该函数取两个参数，主要是 **y_true** (实际值)和 **y_pred** (分类器返回的目标值)。下面是它的代码:

```

#Creating the Confusion matrix
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test, y_pred)

```

**输出:**

![Support Vector Machine Algorithm](../Images/5d409f81c97a746ee8c4c4e2ba2fe440.png)

正如我们在上面的输出图像中看到的，有 66+24= 90 个正确的预测和 8+2= 10 个正确的预测。因此，我们可以说，与逻辑回归模型相比，我们的 SVM 模型有所改进。

*   **可视化训练集结果:**
    现在我们将可视化训练集结果，下面是它的代码:

```

from matplotlib.colors import ListedColormap
x_set, y_set = x_train, y_train
x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),
nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))
mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
alpha = 0.75, cmap = ListedColormap(('red', 'green')))
mtp.xlim(x1.min(), x1.max())
mtp.ylim(x2.min(), x2.max())
for i, j in enumerate(nm.unique(y_set)):
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
        c = ListedColormap(('red', 'green'))(i), label = j)
mtp.title('SVM classifier (Training set)')
mtp.xlabel('Age')
mtp.ylabel('Estimated Salary')
mtp.legend()
mtp.show()

```

**输出:**

通过执行上述代码，我们将获得如下输出:

![Support Vector Machine Algorithm](../Images/89131133ea315860430cfb601ce3c7df.png)

我们可以看到，上面的输出看起来类似于逻辑回归输出。在输出中，我们得到了直线作为超平面，因为我们在分类器中使用了**线性核。上面我们也讨论过，对于二维空间，SVM 的超平面是一条直线。**

*   **可视化测试集结果:**

```

#Visulaizing the test set result
from matplotlib.colors import ListedColormap
x_set, y_set = x_test, y_test
x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),
nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))
mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
alpha = 0.75, cmap = ListedColormap(('red','green' )))
mtp.xlim(x1.min(), x1.max())
mtp.ylim(x2.min(), x2.max())
for i, j in enumerate(nm.unique(y_set)):
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
        c = ListedColormap(('red', 'green'))(i), label = j)
mtp.title('SVM classifier (Test set)')
mtp.xlabel('Age')
mtp.ylabel('Estimated Salary')
mtp.legend()
mtp.show()

```

**输出:**

通过执行上述代码，我们将获得如下输出:

![Support Vector Machine Algorithm](../Images/070b6a1918093bc80d6a3237c93bc0b7.png)

正如我们在上面的输出图像中看到的，SVM 分类器已经将用户分成了两个区域(已购买或未购买)。购买 SUV 的用户位于红色分散点的红色区域。而没有购买 SUV 的用户则在绿色区域，有绿色散点。超平面将两类变量分为已购买变量和未购买变量。

* * *