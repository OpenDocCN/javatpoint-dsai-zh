# 机器学习中的简单线性回归

> 原文:[https://www . javatpoint . com/simple-linear-回归-in-machine-learning](https://www.javatpoint.com/simple-linear-regression-in-machine-learning)

简单线性回归是一种回归算法，它模拟因变量和单个自变量之间的关系。简单线性回归模型显示的关系是线性或倾斜直线，因此称为简单线性回归。

简单线性回归的关键点在于 ***因变量必须是连续/实值*** 。然而，自变量可以用连续值或分类值来衡量。

简单线性回归算法主要有两个目标:

*   **模拟两个变量之间的关系。**如收入与支出、经验与工资的关系等。
*   **预测新观测值。**如根据温度预测天气，根据一年的投资预测公司的收入等。

## 简单线性回归模型:

简单线性回归模型可以用下面的等式表示:

```
y= a0+a1x+ ε 

```

哪里，

**a0=它是回归线的截距(可以通过放 x=0 得到)**
**a1=它是回归线的斜率，它告诉线是增加还是减少。**
**ε =误差项。(对于一个好的模型，它可以忽略不计)**

## 用 Python 实现简单线性回归算法

**简单线性回归的问题陈述示例:**

这里我们取一个有两个变量的数据集:工资(因变量)和经验(自变量)。这个问题的目标是:

*   **我们想知道这两个变量之间有没有相关性**
*   **我们将找到数据集的最佳拟合线。**
*   **因变量如何通过改变自变量而改变。**

在本节中，我们将创建一个简单的线性回归模型，找出代表这两个变量之间关系的最佳拟合线。

要使用 Python 在机器学习中实现简单线性回归模型，我们需要遵循以下步骤:

**第一步:数据预处理**

创建简单线性回归模型的第一步是[数据预处理](data-preprocessing-machine-learning)。我们在本教程的前面已经做过了。但是会有一些变化，这些变化在下面的步骤中给出:

*   首先，我们将导入三个重要的库，这将有助于我们加载数据集、绘制图表和创建简单线性回归模型。

```

import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd

```

*   接下来，我们将数据集加载到我们的代码中:

```

data_set= pd.read_csv('Salary_Data.csv')

```

通过执行上面的代码行(ctrl+ENTER)，我们可以通过单击变量资源管理器选项来读取 Spyder IDE 屏幕上的数据集。

![Simple Linear Regression in Machine Learning](../Images/04b61633114e0ed5927d3bc9d0640838.png)

上面的输出显示了数据集，它有两个变量:工资和经验。

#### 注意:在 Spyder IDE 中，包含代码文件的文件夹必须保存为工作目录，数据集或 csv 文件应在同一文件夹中。

*   之后，我们需要从给定的数据集中提取因变量和自变量。自变量是工作年限，因变量是工资。下面是它的代码:

```

x= data_set.iloc[:, :-1].values
y= data_set.iloc[:, 1].values 

```

在上面几行代码中，对于 x 变量，我们取了-1 值，因为我们想从数据集中删除最后一列。对于 y 变量，我们采用 1 个值作为参数，因为我们想要提取第二列，索引从零开始。

通过执行上面的代码行，我们将得到 X 和 Y 变量的输出，如下所示:

![Simple Linear Regression in Machine Learning](../Images/14e95a5410e6eb0819b312302dd97cf9.png)

在上面的输出图像中，我们可以看到已经从给定的数据集中提取了 X(独立)变量和 Y(从属)变量。

*   接下来，我们将把这两个变量分成测试集和训练集。我们有 30 个观察值，所以我们将对训练集进行 20 个观察值，对测试集进行 10 个观察值。我们正在分割数据集，以便可以使用训练数据集训练模型，然后使用测试数据集测试模型。这方面的代码如下:

```

# Splitting the dataset into training and test set.
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 1/3, random_state=0)

```

通过执行上述代码，我们将获得 x-test、x-train 和 y-test、y-train 数据集。考虑以下图像:

**测试数据集:**

![Simple Linear Regression in Machine Learning](../Images/7ac179e4680f7252548308ba78b917ff.png)

**训练数据集:**

![Simple Linear Regression in Machine Learning](../Images/b4fbc206beb49107e35572b8838a9900.png)

*   对于简单的线性回归，我们将不使用特征缩放。因为 Python 库在某些情况下会处理它，所以我们不需要在这里执行它。现在，我们的数据集已经为处理它做好了充分的准备，我们将开始为给定的问题构建一个简单的线性回归模型。

**步骤 2:将简单线性回归拟合到训练集:**

现在第二步是使我们的模型适合训练数据集。为此，我们将从 **scikit learn** 导入**线性 _ 模型**库的**线性回归**类。导入类后，我们将创建一个名为**回归器**的类对象。这方面的代码如下:

```

#Fitting the Simple Linear Regression model to the training dataset
from sklearn.linear_model import LinearRegression
regressor= LinearRegression()
regressor.fit(x_train, y_train)

```

在上面的代码中，我们使用了 **fit()** 方法来将我们的简单线性回归对象拟合到训练集。在 fit()函数中，我们传递了 x_train 和 y_train，这是我们的因变量和自变量的训练数据集。我们已经将我们的回归对象与训练集进行了拟合，因此模型可以轻松地学习预测变量和目标变量之间的相关性。执行完上面几行代码后，我们将得到下面的输出。

**输出:**

```
Out[7]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)

```

**第三步。测试集结果预测:**

因变量(工资)和自变量(经验)。所以，现在，我们的模型已经准备好预测新观测的输出。在这一步中，我们将向模型提供测试数据集(新的观测值)，以检查它是否能够预测正确的输出。

我们将创建一个预测向量 **y_pred** 和 **x_pred** ，分别包含测试数据集的预测和训练集的预测。

```

#Prediction of Test and Training set result
y_pred= regressor.predict(x_test)
x_pred= regressor.predict(x_train)

```

在执行上述代码行时，将在变量资源管理器选项中生成两个名为 y_pred 和 x_pred 的变量，其中包含训练集和测试集的薪资预测。

**输出:**

您可以通过单击 IDE 中的变量资源管理器选项来检查变量，也可以通过比较 y_pred 和 y_test 的值来比较结果。通过比较这些值，我们可以检查我们的模型表现有多好。

**第四步。可视化训练集结果:**

现在在这一步中，我们将可视化训练集结果。为此，我们将使用 pyplot 库的散点()函数，我们已经在预处理步骤中导入了该函数。**散点图()功能**将创建观察值的散点图。

在 x 轴上，我们将绘制员工的经验年限，在 y 轴上，我们将绘制员工的工资。在函数中，我们将传递训练集的真实值，这意味着一年的经验 x_train，薪水 y_train 的训练集，以及观察值的颜色。这里我们用绿色来观察，但是根据选择，它可以是任何颜色。

现在，我们需要绘制回归线，为此，我们将使用 pyplot 库的 **plot()函数**。在这个函数中，我们将传递训练集的经验年数、训练集 x_pred 的预测工资和线条的颜色。

接下来，我们将给出剧情的标题。所以在这里，我们将使用 **pyplot** 库的 **title()** 函数，并传递名称(“薪资 vs 经验(训练数据集)”。

之后，我们将使用 **xlabel()和 ylabel()函数**为 x 轴和 y 轴分配标签。

最后，我们将使用 show()在图形中表示上述所有内容。代码如下:

```

mtp.scatter(x_train, y_train, color="green") 
mtp.plot(x_train, x_pred, color="red")  
mtp.title("Salary vs Experience (Training Dataset)")
mtp.xlabel("Years of Experience")
mtp.ylabel("Salary(In Rupees)")
mtp.show() 

```

**输出:**

通过执行上面的代码行，我们将得到下面的图表作为输出。

![Simple Linear Regression in Machine Learning](../Images/f0e9152671ef18fab6c0bf172d31f1ec.png)

在上面的图中，我们可以看到绿色圆点中的真实值观测值，红色回归线覆盖了预测值。回归线显示因变量和自变量之间的相关性。

通过计算实际值和预测值之间的差异，可以观察到直线的良好拟合。但是从上面的**图中我们可以看到，大部分的观测值都接近于回归线，因此我们的模型对于训练集**是有好处的。

**第五步。可视化测试集结果:**

在上一步中，我们已经在训练集上可视化了模型的性能。现在，我们将对测试集进行同样的操作。完整的代码将保持与上面的代码相同，除了在这里，我们将使用 x_test，和 y_test 代替 x_train 和 y_train。

这里我们也改变了观测值和回归线的颜色来区分这两个图，但是它是可选的。

```

#visualizing the Test set results
mtp.scatter(x_test, y_test, color="blue") 
mtp.plot(x_train, x_pred, color="red")  
mtp.title("Salary vs Experience (Test Dataset)")
mtp.xlabel("Years of Experience")
mtp.ylabel("Salary(In Rupees)")
mtp.show()

```

**输出:**

通过执行上面的代码行，我们将获得如下输出:

![Simple Linear Regression in Machine Learning](../Images/cf2b78adda7dd2f5b5a66174b52d514b.png)

在上图中，有蓝色给出的观测值，红色回归线给出的预测值。正如我们所看到的，大多数观测值都接近回归线，因此我们可以说我们的简单线性回归是一个很好的模型，能够做出很好的预测。

* * *