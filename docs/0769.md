# 随机森林算法

> 原文:[https://www . javatpoint . com/机器学习-随机-森林-算法](https://www.javatpoint.com/machine-learning-random-forest-algorithm)

随机森林是一种流行的机器学习算法，属于监督学习技术。它可以用于 ML 中的分类和回归问题。它基于**集成学习的概念，**集成学习是*组合多个分类器来解决复杂问题并提高模型性能的过程。*

顾名思义， ***“随机森林(Random Forest)是一个分类器，它包含给定数据集的各个子集上的大量决策树，并取平均值以提高该数据集的预测精度。”*** 随机森林不依赖于一棵决策树，而是从每棵树上获取预测，并基于预测的多数票，预测最终输出。

**森林中的树木数量越多，精度越高，防止了过拟合的问题。**

下图解释了随机森林算法的工作原理:

![Random Forest Algorithm](../Images/d50693b415b0edbdf9c008621074b8fa.png)

#### 注意:为了更好地理解随机森林算法，您应该了解决策树算法。

## 随机森林的假设

因为随机森林结合了多棵树来预测数据集的类别，所以一些决策树可能预测正确的输出，而其他的可能不预测。但是所有的树一起预测正确的输出。因此，下面是更好的随机森林分类器的两个假设:

*   数据集的特征变量中应该有一些实际值，以便分类器可以预测准确的结果，而不是猜测的结果。
*   每棵树的预测必须有非常低的相关性。

## 为什么使用随机森林？

以下几点解释了为什么我们应该使用随机森林算法:

<lirandom forest="" combines="" multiple="multiple" decision="" trees="" hence="" reduces="" the="" risk="" of="" overfitting.="" li="">*   与其他算法相比，它需要更少的训练时间。*   它以高精度预测输出，即使对于它高效运行的大型数据集。*   当大部分数据丢失时，它也可以保持准确性。</lirandom>

## 随机森林算法是如何工作的？

随机森林分两个阶段工作，第一阶段是通过组合 N 棵决策树来创建随机森林，第二阶段是对第一阶段创建的每棵树进行预测。

工作过程可以用下面的步骤和图表来解释:

**步骤-1:** 从训练集中随机选择 K 个数据点。

**步骤 2:** 构建与所选数据点(子集)相关联的决策树。

**第三步:**为要构建的决策树选择数字 N。

**第 4 步:**重复第 1 步& 2。

**步骤-5:** 对于新的数据点，找到每个决策树的预测，将新的数据点分配给赢得多数票的类别。

通过下面的例子可以更好地理解算法的工作原理:

**示例:**假设有一个数据集包含多个水果图像。因此，这个数据集被交给随机森林分类器。数据集被划分为子集，并提供给每个决策树。在训练阶段，每棵决策树都会产生一个预测结果，当一个新的数据点出现时，随机森林分类器会根据大多数结果来预测最终的决策。请考虑下图:

![Random Forest Algorithm](../Images/0e7071189f079305e1ee71c7c5b4009a.png)

## 随机森林的应用

随机森林主要用于四个部门:

1.  **银行业:**银行业大多采用这种算法进行贷款风险的识别。
2.  **医学:**借助这种算法，可以识别疾病趋势和疾病风险。
3.  **土地利用:**通过该算法可以识别出土地利用相似的区域。
4.  **营销:**使用该算法可以识别营销趋势。

## 随机森林的优势

*   随机森林能够执行分类和回归任务。
*   它能够处理高维的大数据集。
*   提高了模型的准确性，防止了过拟合问题。

## 随机森林的缺点

*   虽然随机森林可以用于分类和回归任务，但它并不更适合回归任务。

## 随机森林算法的 Python 实现

现在我们将使用 Python 实现随机森林算法树。为此，我们将使用相同的数据集“user_data.csv”，这是我们在以前的分类模型中使用过的。通过使用相同的数据集，我们可以将随机森林分类器与其他分类模型如[决策树分类器、](machine-learning-decision-tree-classification-algorithm)[【KNN】](https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning)[【SVM】]( https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm)[逻辑回归、]( https://www.javatpoint.com/logistic-regression-in-machine-learning)等进行比较。

实施步骤如下:

*   数据预处理步骤
*   将随机森林算法拟合到训练集
*   预测测试结果
*   测试结果的准确性(创建混淆矩阵)
*   可视化测试集结果。

### 1.数据预处理步骤:

以下是预处理步骤的代码:

```

# importing libraries
import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd

#importing datasets
data_set= pd.read_csv('user_data.csv')

#Extracting Independent and dependent Variable
x= data_set.iloc[:, [2,3]].values
y= data_set.iloc[:, 4].values

# Splitting the dataset into training and test set.
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)

#feature Scaling
from sklearn.preprocessing import StandardScaler  
st_x= StandardScaler()  
x_train= st_x.fit_transform(x_train)  
x_test= st_x.transform(x_test)  

```

在上面的代码中，我们已经对数据进行了预处理。我们加载数据集的位置，如下所示:

![Random Forest Algorithm](../Images/196ed2363dde22f9f0d527f122b13c1b.png)

### 2.将随机森林算法拟合到训练集:

现在我们将随机森林算法拟合到训练集。为了适合它，我们将从 **sklearn.ensemble** 库中导入 **RandomForestClassifier** 类。代码如下:

```

#Fitting Decision Tree classifier to the training set
from sklearn.ensemble import RandomForestClassifier
classifier= RandomForestClassifier(n_estimators= 10, criterion="entropy")
classifier.fit(x_train, y_train)

```

在上面的代码中，分类器对象采用以下参数:

*   **n _ estimates =**随机森林中所需的树木数量。默认值为 10。我们可以选择任何数量，但需要注意过度拟合的问题。
*   **判据=** 是分析拆分精度的函数。这里我们把“熵”作为信息增益。

**输出:**

```
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',
                       max_depth=None, max_features='auto', max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=10,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)

```

### 3.预测测试集结果

由于我们的模型适合训练集，所以现在我们可以预测测试结果。对于预测，我们将创建一个新的预测向量 y_pred。下面是它的代码:

```

#Predicting the test set result
y_pred= classifier.predict(x_test)

```

**输出:**

预测向量如下所示:

![Random Forest Algorithm](../Images/0210a1e6aa0b430af61bc696715369cd.png)

通过检查上述预测向量和测试集真实向量，我们可以确定分类器所做的错误预测。

### 4.创建混淆矩阵

现在我们将创建混淆矩阵来确定正确和不正确的预测。下面是它的代码:

```

#Creating the Confusion matrix
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test, y_pred)

```

**输出:**

![Random Forest Algorithm](../Images/d26400b5c7a6a48250b433310cb63097.png)

从上面的矩阵中我们可以看到，有 **4+4= 8 个不正确的预测**和 **64+28= 92 个正确的预测。**

### 5.可视化训练集结果

这里我们将可视化训练集结果。为了可视化训练集结果，我们将绘制随机森林分类器的图表。分类器会像我们在[逻辑回归中所做的那样，为已经购买或未购买 SUV 汽车的用户预测是或否。](https://www.javatpoint.com/logistic-regression-in-machine-learning)下面是它的代码:

```

from matplotlib.colors import ListedColormap
x_set, y_set = x_train, y_train
x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),
nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))
mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
alpha = 0.75, cmap = ListedColormap(('purple','green' )))
mtp.xlim(x1.min(), x1.max())
mtp.ylim(x2.min(), x2.max())
for i, j in enumerate(nm.unique(y_set)):
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
        c = ListedColormap(('purple', 'green'))(i), label = j)
mtp.title('Random Forest Algorithm (Training set)')
mtp.xlabel('Age')
mtp.ylabel('Estimated Salary')
mtp.legend()
mtp.show()

```

**输出:**

![Random Forest Algorithm](../Images/2d710e489f3b580c3cfd427232055779.png)

上图是使用训练集结果的随机森林分类器的可视化结果。它非常类似于决策树分类器。每个数据点对应于 user_data 的每个用户，紫色和绿色区域是预测区域。紫色区域为未购买 SUV 汽车的用户分类，绿色区域为购买 SUV 的用户分类。

因此，在随机森林分类器中，我们选择了 10 棵树，这些树预测了购买变量的是或否。分类器接受大部分预测并提供结果。

### 6.可视化测试集结果

现在我们将可视化测试集结果。下面是它的代码:

```

#Visulaizing the test set result
from matplotlib.colors import ListedColormap
x_set, y_set = x_test, y_test
x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),
nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))
mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
alpha = 0.75, cmap = ListedColormap(('purple','green' )))
mtp.xlim(x1.min(), x1.max())
mtp.ylim(x2.min(), x2.max())
for i, j in enumerate(nm.unique(y_set)):
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
        c = ListedColormap(('purple', 'green'))(i), label = j)
mtp.title('Random Forest Algorithm(Test set)')
mtp.xlabel('Age')
mtp.ylabel('Estimated Salary')
mtp.legend()
mtp.show()

```

**输出:**

![Random Forest Algorithm](../Images/833c7403b01d0e2d61b973e684386d6f.png)

上图是测试集的可视化结果。我们可以检查是否有最少数量的不正确预测(8)，而没有过度拟合问题。通过改变分类器中的树的数量，我们会得到不同的结果。

* * *