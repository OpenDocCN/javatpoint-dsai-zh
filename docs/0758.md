# 最大似然多项式回归

> 原文:[https://www . javatpoint . com/机器学习-多项式-回归](https://www.javatpoint.com/machine-learning-polynomial-regression)

*   多项式回归是一种将因变量(y)和自变量(x)之间的关系建模为 n 次多项式的回归算法。多项式回归方程如下所示:

```
y= b0+b1x1+ b2x12+ b2x13+...... bnx1n

```

*   也称为 ML 中多元线性回归的特例。因为我们在多元线性回归方程中加入了一些多项式项，将其转化为多项式回归。
*   这是一个线性模型，为了提高精度做了一些修改。
*   多项式回归中用于训练的数据集是非线性的。
*   它利用线性回归模型来拟合复杂的非线性函数和数据集。
*   **因此，*“在多项式回归中，原始特征被转换成所需次数的多项式特征(2，3，..，n)，然后使用线性模型建模。”*T3】**

## 多项式回归的需求:

ML 中多项式回归的需要可以从以下几点理解:

*   如果我们在**线性数据集**上应用线性模型，那么它为我们提供了一个很好的结果，就像我们在简单线性回归中看到的那样，但是如果我们在**非线性数据集**上应用相同的模型而没有任何修改，那么它将产生剧烈的输出。由于哪个损失函数会增加，错误率会很高，准确性会降低。
*   所以对于这样的情况，**数据点以非线性方式排列，我们需要多项式回归模型**。使用下面的线性数据集和非线性数据集的对比图，我们可以更好地理解它。

![ML Polynomial Regression](../Images/cfcefe2696c5567fc2403698a0f8e9da.png)

*   在上图中，我们获取了一个非线性排列的数据集。因此，如果我们试图用线性模型覆盖它，那么我们可以清楚地看到它几乎不覆盖任何数据点。另一方面，曲线适合覆盖大多数数据点，这是多项式模型。
*   因此，*如果数据集以非线性方式排列，那么我们应该使用多项式回归模型，而不是简单线性回归。*

#### 注:多项式回归算法也称为多项式线性回归，因为它不依赖于变量，而是依赖于系数，系数以线性方式排列。

## 多项式回归模型的方程:

**简单线性回归方程:y = b <sub>0</sub> +b <sub>1</sub> x.........(一)**

**多元线性回归方程:y = b<sub>0</sub>+b<sub>1</sub>x+b<sub>2</sub>x<sub>2</sub>+b<sub>3</sub>x<sub>3</sub>+....+ b <sub>n</sub> x <sub>n</sub> .........**

**多项式回归方程:y = b<sub>0</sub>+b<sub>1</sub>x+b<sub>2</sub>x<sup>2</sup>+b<sub>3</sub>x<sup>3</sup>+....+ b <sub>n</sub> x <sup>n</sup> ..........**

当我们比较上述三个方程时，我们可以清楚地看到，这三个方程都是多项式方程，但变量的次数不同。简单和多元线性方程也是一次多项式方程，多项式回归方程是第 n 次线性方程。所以如果我们给我们的线性方程增加一个度数，那么它将被转换成多项式线性方程。

#### 注:要更好地理解多项式回归，必须具备简单线性回归的知识。

## 使用 Python 实现多项式回归:

这里我们将使用 Python 实现多项式回归。我们将通过比较多项式回归模型和简单线性回归模型来理解它。首先，让我们理解我们要为其构建模型的问题。

**问题描述:**有一家人力资源公司，要招聘新的人选。候选人告诉了他以前的年薪 16 万，人力资源必须检查他说的是实话还是虚张声势。因此，为了识别这一点，他们只有一个他以前公司的数据集，其中前 10 个职位的工资与他们的水平一起被提及。通过检查可用的数据集，我们发现职位级别和工资之间存在**非线性关系。我们的目标是建立一个**虚张声势检测器回归**模型，这样人力资源就可以雇佣一个诚实的候选人。下面是建立这样一个模型的步骤。**

![ML Polynomial Regression](../Images/59b53230cc0893c9a711e8b60944367b.png)

## 多项式回归的步骤:

多项式回归的主要步骤如下:

*   数据预处理
*   构建一个线性回归模型，并使其适合数据集
*   构建多项式回归模型，并将其与数据集进行拟合
*   可视化线性回归和多项式回归模型的结果。
*   预测产量。

#### 注意:在这里，我们将建立线性回归模型以及多项式回归，以查看预测之间的结果。线性回归模型可供参考。

**数据预处理步骤:**

除了一些变化之外，数据预处理步骤将保持与以前的回归模型相同。在多项式回归模型中，我们不会使用特征缩放，也不会将数据集分成训练集和测试集。这有两个原因:

*   数据集包含的信息非常少，不适合将其分为测试集和训练集，否则我们的模型将无法找到工资和级别之间的相关性。
*   在这个模型中，我们希望对工资有非常准确的预测，所以模型应该有足够的信息。

预处理步骤的代码如下:

```

# importing libraries
import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd

#importing datasets
data_set= pd.read_csv('Position_Salaries.csv')

#Extracting Independent and dependent Variable
x= data_set.iloc[:, 1:2].values
y= data_set.iloc[:, 2].values

```

**说明:**

*   在上面几行代码中，我们已经导入了重要的 Python 库来导入数据集并对其进行操作。
*   接下来，我们导入了数据集“**Position _ sales . CSV**”，它包含三列(职位、级别和薪资)，但我们将只考虑两列(薪资和级别)。
*   之后，我们从数据集中提取了因变量和自变量。对于 x 变量，我们将参数取为[:，1:2]，因为我们想要 1 个索引(级别)，并包含:2，使其成为矩阵。

**输出:**

通过执行上述代码，我们可以将数据集读取为:

![ML Polynomial Regression](../Images/0ec88db692f0d525f51cfc3461242daf.png)

正如我们在上面的输出中看到的，有三列(职位、级别和薪水)。但是我们只考虑两列，因为位置相当于级别，或者可以被视为位置的编码形式。

这里我们将预测第 **6.5** 级的输出，因为候选人有 4 年以上的区域经理经验，所以他必须在第 7 级和第 6 级之间。

**建立线性回归模型:**

现在，我们将构建线性回归模型并将其拟合到数据集。在建立多项式回归时，我们将以线性回归模型为参考，并比较两者的结果。代码如下:

```

#Fitting the Linear Regression to the dataset
from sklearn.linear_model import LinearRegression
lin_regs= LinearRegression()
lin_regs.fit(x,y)

```

在上面的代码中，我们使用**线性回归**类的 **lin_regs** 对象创建了简单线性模型，并将其拟合到数据集变量(x 和 y)。

**输出:**

```
Out[5]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)

```

**建立多项式回归模型:**

现在我们将建立多项式回归模型，但它与简单线性模型略有不同。因为这里我们将使用**预处理**库的**多项式特征**类。我们正在使用这个类向数据集添加一些额外的特征。

```

 #Fitting the Polynomial regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
poly_regs= PolynomialFeatures(degree= 2)
x_poly= poly_regs.fit_transform(x)
lin_reg_2 =LinearRegression()
lin_reg_2.fit(x_poly, y)

```

在上面几行代码中，我们使用了**poly _ regs . fit _ transform(x)**，因为首先我们将我们的特征矩阵转换为多项式特征矩阵，然后将其拟合到多项式回归模型中。参数值(度数= 2)取决于我们的选择。我们可以根据我们的多项式特征来选择它。

执行完代码后，我们会得到另一个矩阵 **x_poly** ，可以在变量浏览器选项下看到:

![ML Polynomial Regression](../Images/979cd97186189fcfd449241ce0f4f592.png)

接下来，我们使用另一个线性回归对象，即 **lin_reg_2** ，将我们的 **x_poly** 向量拟合到线性模型。

**输出:**

```
Out[11]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)

```

**可视化线性回归结果:**

现在，我们将像在简单线性回归中一样，可视化线性回归模型的结果。下面是它的代码:

```

#Visulaizing the result for Linear Regression model
mtp.scatter(x,y,color="blue")
mtp.plot(x,lin_regs.predict(x), color="red")
mtp.title("Bluff detection model(Linear Regression)")
mtp.xlabel("Position Levels")
mtp.ylabel("Salary")
mtp.show()

```

**输出:**

![ML Polynomial Regression](../Images/d110dadf9117bbbb4944005af43b993d.png)

在上面的输出图像中，我们可以清楚地看到回归线离数据集如此之远。预测在红色直线上，蓝色点是实际值。如果我们考虑这个输出来预测首席执行官的价值，它将给出大约。60 万美元，这与真实价值相差甚远。

所以我们需要一个曲线模型来拟合数据集，而不是直线。

**可视化多项式回归的结果**

在这里，我们将可视化多项式回归模型的结果，其代码与上述模型几乎没有不同。

这方面的代码如下:

```

#Visulaizing the result for Polynomial Regression
mtp.scatter(x,y,color="blue")
mtp.plot(x, lin_reg_2.predict(poly_regs.fit_transform(x)), color="red")
mtp.title("Bluff detection model(Polynomial Regression)")
mtp.xlabel("Position Levels")
mtp.ylabel("Salary")
mtp.show()

```

在上面的代码中，我们采用了 Lin _ reg _ 2 . predict(poly _ regs . fit _ transform(x)，而不是 x_poly，因为我们想要一个线性回归器对象来预测多项式特征矩阵。

**输出:**

![ML Polynomial Regression](../Images/7b028f4d557683cbc67107696361dd99.png)

正如我们在上面的输出图像中看到的，预测值接近真实值。上面的情节会随着我们改变程度而变化。

**对于度数= 3:**

如果我们改变度数=3，那么我们会给出一个更精确的图，如下图所示。

![ML Polynomial Regression](../Images/af68912e94eb23848d7e936934827258.png)

因此，正如我们在上面的输出图像中看到的，6.5 级的预测工资接近 17 万美元-19 万美元，这似乎表明未来的员工在说他工资的真相。

**度数= 4:** 我们再把度数改成 4，现在会得到最准确的图。因此，我们可以通过增加多项式的次数来得到更精确的结果。

![ML Polynomial Regression](../Images/86e7162c01cf6f18a8e88dfe178f93a1.png)

**用线性回归模型预测最终结果:**

现在，我们将使用线性回归模型预测最终输出，看看员工是在说真话还是虚张声势。因此，为此，我们将使用**预测()**方法，并将传递值 6.5。下面是它的代码:

```

lin_pred = lin_regs.predict([[6.5]])
print(lin_pred)

```

**输出:**

```
[330378.78787879]

```

**用多项式回归模型预测最终结果:**

现在，我们将使用多项式回归模型来预测最终输出，并与线性模型进行比较。下面是它的代码:

```

poly_pred = lin_reg_2.predict(poly_regs.fit_transform([[6.5]]))
print(poly_pred)

```

**输出:**

```
[158862.45265153]

```

如我们所见，多项式回归的预测输出是[158862.45265153]，这更接近真实值。因此，我们可以说未来的员工说的是真的。

* * *