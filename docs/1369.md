# Keras核层

> 原文：<https://www.javatpoint.com/keras-core-layers>

## 稠密的

```

keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

```

密集层可以定义为密集连接的公共神经网络层。**输出=激活(点(输入，内核)+偏置)**操作由密集层执行。这里，激活正在执行元素方式的激活功能，为了传递激活参数，层构建了一个称为**核**的权重矩阵，**偏置**是层创建的向量，仅当 **use_bias** 为 **True** 时才适用。

需要注意的是，如果给该层的输入具有大于 2 的等级，它将被压平到其与内核的主点积。

**例**

```

# First layer in the sequential model:
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# The model takes the input as arrays of shape (*, 16) and output arrays of shape (*, 32)
# After the first layer, you don't need to specify the size of the input:
model.add(Dense(32))

```

**论据**

*   **单位:**表示承认输出空间维度的正整数。
*   **激活:**确保致密层利用元素态激活功能。这是线性激活，默认情况下设置为无。由于其线性度有限，我们没有太多内置的激活功能。
*   **use_bias:** 这是一个可选参数，这意味着我们可能会也可能不会将其纳入我们的计算中。它表示一个布尔值，显示图层是否使用了偏移向量。
*   **内核初始化器:**可以定义为**内核**权重矩阵的初始化器。
*   **bias_initializer:** 可以定义为默认情况下 Keras 使用零初始值的偏置向量的初始值。假设它将偏置向量设置为全零。
*   **核 _ 正则化器:**可以称为正则化函数，在**核**权重矩阵上实现。
*   **bias _ regulator:**可以定义为正则化函数，应用于偏置向量。
*   **activity _ regulator:**它与应用于层的输出(其激活)的正则化函数相关。
*   **kernel_constraint:** 是指约束，应用于核权重矩阵。
*   **bias_constraint:** 可以定义为约束，应用于偏置向量。

**输入形状**

输入形状图层接受形状张量 **(batch_size，…，input_dim)** ，并确保其最常见的情况必须是包含形状 **(batch_size，input_dim)** 的 **2D** 输入。

**输出形状**

它输出形状张量**(批量大小，…，单位)**。例如，如果**输入的**是形状为**(批次大小，输入尺寸)**的 2D，则相应的**输出的**将是形状为**(批次大小，单位)**。

## 激活

```

keras.layers.Activation(activation)

```

这是在输出上实现激活功能的层。

**论据**

*   **激活:**基本上是指要使用的一个激活函数的名字，或者简单的我们可以说是一个 antio 或者 TensorFlow 操作。

**输入形状**

它由任意输入形状组成。它使用名为 **input_shape** 的参数，同时将其用作模型中的初始层。input_shape 可以定义为不包括样本轴的整数元组。

**输出形状**

输出形状与输入形状相同。

## 拒绝传统社会的人

```

keras.layers.Dropout(rate, noise_shape=None, seed=None)

```

丢弃应用于输入，因为它通过在每次更新的训练时间内将分数率的单位随机设置为 0 来防止过拟合。

**论据**

*   **速率:**是指 0 到 1 之间的浮点值，代表要降的分数单位。
*   **noise_shape:** 它指的是一个一维张量整数，它是二进制丢失掩码形状的缩影，将用于与输入相乘。如果输入形状是**(批处理大小，时间步长，特征)**，并且对于所有时间步长，您希望您的丢失掩码相似，那么，在这种情况下，可以使用 **noise_shape=(批处理大小，1，特征)**。
*   **种子:**表示将作为随机种子使用的 python 整数。

## 变平

```

keras.layers.Flatten(data_format=None)

```

展平层用于在不影响批次大小的情况下展平输入。

**论据**

*   **数据 _ 格式:**可以定义为**通道 _ 最后**(默认)或**通道 _ 第一**中的一个字符串。它主要用于对输入维度进行排序，以便在模型从一种数据格式切换到另一种数据格式时保留权重的排序。这里**通道 _ 最后**涉及**(批次、…、通道)**的输入形状，而**通道 _ 首先**涉及**(批次、通道、…)** 的输入形状。默认情况下，在 Keras 配置文件中找到的 **image_data_format** 值位于 **~/。keras/keras.json** ，否则如果还没有设置，那么会在“ **channels_last** ”。

**例**

```

model = Sequential()
model.add(Conv2D(64, (3, 3),
                 input_shape=(3, 32, 32), padding='same',))
# Now: model.output_shape == (None, 64, 32, 32)

model.add(Flatten())
# Now: model.output_shape == (None, 65536)

```

## 投入

```

keras.engine.input_layer.Input()

```

输入层使用 **Input()** 来实例化一个 Keras 张量，它只是来自后端的一个张量对象，比如 antao、TensorFlow 或 CNTK。它可以用一些特定的属性来扩充，这将让我们只借助输入和输出来构建一个 Keras 模型。

如果我们有 m，n 和 o 个 Keras 张量，那么我们可以执行 model = Model(输入=[m，n]，输出=o)。

其他一些增加的 Keras 属性有； **_keras_shape** ，通过 keras 侧形状推断传播的整数形状元组， **_keras_history** 是最后一层，应用在张量上。最后一层允许递归检索整个层图。

**论据**

*   **形状:**形状元组可以定义为不包括批次大小的整数。例如， **shape=(32)，**指定预期的输入批次将是 32 维向量。
*   **batch_shape:** 形状元组指示包括批次大小的整数，使得例如， **batch_shape=(10，32)** 表示预期的输入批次将是十个 32 维向量， **batch_shape=(无，32)** 表示任意数量的 32 维向量的批次。
*   **名称:**是图层的可选字符串名称，必须唯一，即使不提供，也会自动生成。
*   **数据类型:**输入的预期数据类型是一个字符串 **(float32，float64，int32，…)** 。
*   **稀疏:**它是一个布尔值，指定创建的占位符是否稀疏。
*   **张量:**它是一个可选的张量，存在于输入层中。如果已设置，图层将不会创建占位符张量

**返回**

它返回一个张量。

**例**

```

# Logistic regression in Keras
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)

```

## 使再成形

```

keras.layers.Reshape(target_shape)

```

它用于将输出整形为某种特定的形状。

**论据**

*   **target_shape:** 指指向输出形状的整数元组，不包括批次轴。

**输入形状**

它包括任意输入形状，即使它是固定的，并使用 **input_shape** 参数，同时使用该层作为模型中的初始层。

**输出形状**

```

(batch_size, ) + target_shape

```

**例**

```

# First layer in a Sequential model
model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
# Now: model.output_shape == (None, 3, 4)
# Note: Here `None` represents the batch dimension

# An intermediate layer in a Sequential model
model.add(Reshape((6, 2)))
# Now: model.output_shape == (None, 6, 2)

# It also supports shape inference using `-1` as a dimension
model.add(Reshape((-1, 2, 2)))
# Now: model.output_shape == (None, 3, 2, 2)

```

## 排列

```

keras.layers.Permute(dims)

```

它按照给定的模式排列输入的维度，主要用于将 RNN 和康文网络连接在一起。

**例**

```

model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension

```

**参数**

*   **dims:** 可以定义为整数元组。排列模式不包含样本维度。在这里，索引从 1 开始，对于任何随机实例，(2，1)将置换输入的第一维和第二维。

**输入形状**

它由任意输入形状组成，并使用 **input_shape** 关键字参数，这是一个整数元组。当使用该层作为模型中的初始层时，会用到该参数。它不包括样本轴。

**输出形状**

输出形状类似于输入形状，只是尺寸根据特定模式重新排序。

## 重复向量机

```

keras.layers.RepeatVector(n)

```

RepeatVector 层用于重复输入 n 次。

**例**

```

model = Sequential()
model.add(Dense(32, input_dim=32))
# now: model.output_shape == (None, 32)
# note: `None` is the batch dimension

model.add(RepeatVector(3))
# now: model.output_shape == (None, 3, 32)

```

**论据**

*   **n:** 可以定义为表示重复因子的整数。

**输入形状**

它由形状为**(数量样本，特征)**的 2D 张量组成。

**输出形状**

它构成了形状的三维张量 **(num_samples，n，features)** 。

## 希腊字母的第 11 个

```

keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)

```

该层用于包装任意表达式，如**层**的对象。

**示例**

```

# Adding a x -> x^2 layer
model.add(Lambda(lambda x: x ** 2))

```

```

# Now add a layer that will return the concatenation of the positive part of the input and the opposite of the negative part
def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)

def antirectifier_output_shape(input_shape):
    shape = list(input_shape)
    assert len(shape) == 2  # only valid for 2D tensors
    shape[-1] *= 2
    return tuple(shape)

model.add(Lambda(antirectifier,
                 output_shape=antirectifier_output_shape))

```

```

# Now add a layer that will return the hadamard product and its sum from two input tensors.
def hadamard_product_sum(tensors):
    out1 = tensors[0] * tensors[1]
    out2 = K.sum(out1, axis=-1)
    return [out1, out2]

def hadamard_product_sum_output_shape(input_shapes):
    shape1 = list(input_shapes[0])
    shape2 = list(input_shapes[1])
    assert shape1 == shape2  # else hadamard product isn't possible
    return [tuple(shape1), tuple(shape2[:-1])]

x1 = Dense(32)(input_1)
x2 = Dense(32)(input_2)
layer = Lambda(hadamard_product_sum, hadamard_product_sum_output_shape)
x_hadamard, x_sum = layer([x1, x2])

```

**论据**

*   **函数:**可以定义为函数，需要计算。它以输入张量或张量列表作为第一个参数。
*   **output_shape:** 它从函数本身期望输出形状，如果使用了 antio，这是相关的。如果**输出形状**是一个元组，那么它从第一维开始指定。它假设样本尺寸类似于**输出 _ 形状=(输入 _ 形状[0]，)+输出 _ 形状**，或者输入为**无**。同样，尺寸为**无:output_shape =(无)，+ output_shape** ，如果将一个函数指定给整个形状作为输入形状的函数:**output _ shape = f(input _ shape)**。
*   **掩码:**可以是无，表示没有掩码，也可以是张量，与嵌入的输入掩码相关。
*   **参数:**是传递给函数的可选参数关键字字典。

**输入形状**

输入形状是一个任意的整数元组，它使用参数关键字 input_shape，同时将该层用作模型中的初始层，并且不包括样本轴。

**输出形状**

它或者由 output_shape 参数指定，或者在使用 TensorFlow 或 CNTK 时自动推断。

## 活动规则化

```

keras.layers.ActivityRegularization(l1=0.0, l2=0.0)

```

活动规则层根据输入活动更新成本函数。

**论据**

*   **l1:** L1 是正浮动正则化因子。
*   **l2:** L2 是正浮动正则化因子。

**输入形状**

它是一个任意的整数元组，使用 **input_shape** 参数，同时使用该层作为模型中的初始层。它不包含样本轴

**输出形状**

输出形状与输入形状相似。

## 掩饰

```

keras.layers.Masking(mask_value=0.0)

```

屏蔽层用于屏蔽序列，只需使用屏蔽值来避免时间步长。对于给定的时间步长样本，如果所有要素都等于 **mask_value** ，那么，在这种情况下，只有当样本时间步长支持屏蔽时，才会在所有下游图层中屏蔽(跳过)。

如果任何下游层不支持屏蔽，但仍接收输入屏蔽，则会引发异常。

**例**

假设 **x** 是形状(样本、时间步长、特征)的 numpy 数据数组，它将被馈送到 LSTM 层。现在假设您愿意在时间步长#3 屏蔽#0，在时间步长#5 屏蔽样本#2，因为您缺少这些样本时间步长的特性，那么您可以执行以下操作:

*   设置 **x[0，3，]=0** 和 **x[2，5，]=0** 。
*   要在 LSTM 图层之前插入一个**遮罩**图层，请使用**遮罩 _ 值=0** 。

```

model = Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(LSTM(32))

```

**论据**

*   **mask_value:** 可以是无，也可以是跳过。

## 1D 空间坠落

这是一个空间丢失 1D 版本，执行与丢失相同的功能，但它不会丢失单个元素，而是会丢失整个 1D 要素图。当连续帧在特征图中被强链接时，就像在卷积层中执行的那样，那么，在这种情况下，激活将不会被常规丢弃所规范，而是最终降低有效学习率。在这种特殊情况下，它将有助于促进要素地图之间的独立性，并将被替代使用。

**论据**

*   **速率:**是 0 到 1 之间的浮动。要删除的输入单位的分数。

**输入形状**

它是一个三维形状张量(样本、时间步长、通道)

**输出形状**

输出形状类似于输入形状。

## 二维空间降级

```

 keras.layers.SpatialDropout2D(rate, data_format=None)

```

这是一个空间丢失的 2D 版本。它还执行与退学生类似的功能；但是，它删除了整个 2D 要素地图，而不是一个单独的元素。如果相邻帧在特征图中是强相关的，就像在卷积层中一样，那么激活将不会被规则的丢失所规范，否则将降低有效学习率。在这种情况下，它促进了要素地图之间的独立性，并被替代使用。

**论据**

*   **速率:**是 0 到 1 之间的浮动。要删除的输入单位的一小部分。
*   **数据 _ 格式:**为任一模式，即**【通道 _ 第一】**或**【通道 _ 最后】**。如果它处于**通道 _ 第一**模式，那么深度意味着在索引 1，否则在**通道 _ 最后**的情况下，它在索引 3。默认为**图像 _ 数据 _ 格式**值，可在 **~/的 Keras 配置中找到。keras/keras.json** 。如果您在该文件夹中找不到它，那么它就位于“channels_last”。

**输入形状**

如果**data _ format = ' channels _ first '**，那么 4D 张量的形状将是**(样本、通道、行、列)**，否则如果它是**data _ format**=**' channels _ last '**，那么 4D 张量的形状将是**(样本、行、列、通道)**。

**输出形状**

输出形状类似于输入形状。

## 三维空间降级

```

keras.layers.SpatialDropout3D(rate, data_format=None)

```

它是一个空间丢失的 3D 版本，执行与丢失相似的功能，但它丢失了完整的 3D 要素图，而不是任何特定的元素。如果存在于特征图中的相邻体素像卷积层一样强相关，则常规缺失将不会规范激活，否则将降低有效学习率。它还支持要素地图之间的独立性。

**论据**

*   **汇率:**是 0 到之间的浮动。要删除的输入单位的分数。
*   **data_format:** 有两种模式，即**“channel _ first”**或**“channel _ last”**，这样**“channel _ first”**中的 channel 维度位于索引 1，如果是**“channel _ last”**，则位于索引 4。默认为**图像 _ 数据 _ 格式**值，可在 **~/的 Keras 配置中找到。keras/keras.json** 。如果您在该文件夹中找不到它，那么它就位于“channels_last”。

**输入形状**

5D 张量的形状是:**(样本、通道、dim1、dim2、dim3)**if**data _ format**=**“通道 _ 第一”**，else **(样本、dim1、dim2、dim 3、通道)** if **data_format** = **“通道 _ 最后”**。

**输出形状**

输出形状与输入形状相同。

* * *