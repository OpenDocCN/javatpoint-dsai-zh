# 打包与助推

> 原文:[https://www.javatpoint.com/bagging-vs-boosting](https://www.javatpoint.com/bagging-vs-boosting)

我们都在日常生活中使用决策树技术来做出决定。组织使用像决策树这样的监督机器学习技术来做出更好的决策，并产生更多的盈余和利润。

**集成**方法结合不同的决策树来提供更好的预测结果，然后利用单个决策树。集成模型背后的主要原则是一组弱学习者聚集在一起形成一个活跃的学习者。

下面给出了两种用于执行集成决策树的技术。

## 制袋材料

当我们的目标是减少决策树的方差时，使用 Bagging。这里的概念是从训练样本中创建一些数据子集，这些数据是通过替换随机选择的。现在，每个子集数据集合都被用来准备它们的决策树，因此，我们最终得到了各种模型的集合。使用来自多个树的所有假设的平均值，这比单个决策树更强大。

**随机森林**是对套袋的扩展。预测数据的随机子集还需要一步。它还可以随机选择特征，而不是使用所有特征来开发树。当我们有许多随机的树时，它被称为随机森林。

以下是实现随机林所采取的步骤:

*   让我们考虑训练数据集中的 **X** 观测值 **Y** 特征。首先，从训练数据集中随机抽取一个模型进行替换。
*   这棵树长到了最大。
*   重复给定的步骤，并给出预测，其基于来自 n 个树的预测的集合。

**使用随机森林技术的优势:**

*   它能很好地管理高维数据集。
*   它管理缺失的数量，并保持缺失数据的准确性。

**使用随机森林技术的缺点:**

由于最后的预测依赖于来自子集树的平均预测，它不会给出回归模型的精确值。

## 增压:

助推是另一个收集预测因子的集合程序。换句话说，我们拟合连续的树，通常是随机样本，并且在每一步，目标是解决来自先前树的净误差。

如果一个给定的输入被理论错误分类，那么它的权重就会增加，这样即将到来的假设就更有可能通过合并整个集合来正确分类，最终将弱学习者转化为表现更好的模型。

**梯度**增压是增压程序的扩展。

```

Gradient Boosting = Gradient Descent + Boosting

```

它利用梯度下降算法，可以优化任何可微损失函数。树的集合是单独构建的，并且各个树被连续求和。下一棵树试图恢复损失(这是实际值和预测值之间的差异)。

**使用梯度增强方法的优势:**

*   它支持不同的损失函数。
*   它能很好地进行互动。

**使用梯度增强方法的缺点:**

*   它需要谨慎调整不同的超参数。

## 装袋和助推的区别:

![Bagging vs Boosting](../Images/a2814ab5c64566f5eb75b3cf68654337.png)

| 制袋材料 | 助推 |
| 从整个训练数据集中随机抽取各种训练数据子集进行替换。 | 每个新的子集都包含以前的模型错误分类的组件。 |
| 打包试图解决过度合身的问题。 | 助推试图减少偏见。 |
| 如果分类器不稳定(高方差)，那么我们需要应用 bagging。 | 如果分类器是稳定和直接的(高偏差)，那么我们需要应用提升。 |
| 每种型号都获得同等的重量。 | 模型根据其性能进行加权。 |
| 目的减少方差，而不是偏倚。 | 目的减少偏倚，而不是方差。 |
| 这是连接属于同一类型的预测的最简单方法。 | 这是连接属于不同类型的预测的一种方式。 |
| 每个模型都是独立构建的。 | 新模型会受到之前开发的模型性能的影响。 |

* * *