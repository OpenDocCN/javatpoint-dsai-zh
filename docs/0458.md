# 强化学习教程

> 原文:[https://www.javatpoint.com/reinforcement-learning](https://www.javatpoint.com/reinforcement-learning)

![Reinforcement Learning Tutorial](../Images/faed3822b93107d8092d180753dbf1c4.png)

我们的强化学习教程将为您提供强化学习的完整概述，包括 MDP 和 Q 学习。在 RL 教程中，您将学习以下主题:

*   [什么是强化学习？](#What)
*   [强化学习中使用的术语。](#Terms)
*   [强化学习的关键特征。](#Key-features)
*   [强化学习的要素。](#Elements)
*   [实施强化学习的方法。](#Approaches)
*   [强化学习是如何工作的？](#Work)
*   [贝尔曼方程。](#Bellman)
*   [强化学习的类型。](#Types)
*   [强化学习算法。](#Algorithm)
*   [马尔可夫决策过程。](#Markov)
*   [什么是问学？](#Q-Learning)
*   [监督学习和强化学习的区别。](#Difference)
*   [强化学习的应用。](#Applications)
*   [结论。](#Conclusion)

* * *

## 什么是强化学习？

*   强化学习是一种基于反馈的机器学习技术，在这种技术中，代理通过执行动作和查看动作的结果来学习在环境中的行为。对于每一个好的行为，代理人得到正反馈，对于每一个不好的行为，代理人得到负反馈或惩罚。
*   在强化学习中，与[监督学习不同，代理使用反馈自动学习，不需要任何标记数据。](https://www.javatpoint.com/supervised-machine-learning)
*   由于没有标注数据，所以代理必然只能凭经验学习。
*   RL 解决的是特定类型的问题，其中决策是顺序的，目标是长期的，比如**玩游戏、机器人**等。
*   代理与环境交互并自行探索环境。代理在强化学习中的主要目标是通过获得最大的积极回报来提高性能。
*   代理通过命中和尝试的过程进行学习，并根据经验学习以更好的方式执行任务。因此，我们可以说 ***“强化学习是一种机器学习方法，其中智能代理(计算机程序)与环境交互，并学习在环境中行动。”*** 机器狗如何学习手臂的运动就是强化学习的一个例子。
*   它是[人工智能](https://www.javatpoint.com/artificial-intelligence-tutorial)的核心部分，所有 [AI 智能体](https://www.javatpoint.com/agents-in-ai)都在强化学习的概念上下功夫。在这里，我们不需要对代理进行预编程，因为它是在没有任何人工干预的情况下从自己的经验中学习的。
*   **示例:**假设迷宫环境中存在一个 AI 代理，他的目标是找到钻石。代理通过执行一些动作与环境交互，并且基于这些动作，代理的状态被改变，并且它还接收作为反馈的奖励或惩罚。
*   代理继续做这三件事(**采取行动，改变状态/保持不变状态，获得反馈**，通过做这些行动，他学习和探索环境。
*   代理了解到哪些行为会导致正反馈或奖励，哪些行为会导致负反馈惩罚。作为积极的奖励，代理人得到一个积极的分数，作为惩罚，它得到一个消极的分数。

![What is Reinforcement Learning](../Images/607caada5451a0d559d7885866cad8b0.png)

* * *

## 强化学习中使用的术语

*   **Agent():** 能够感知/探索环境并对其采取行动的实体。
*   **环境():**代理人存在或被代理人包围的情况。在 RL 中，我们假设随机环境，这意味着它本质上是随机的。
*   **动作():**动作是代理在环境中采取的动作。
*   **State():** State 是代理每次动作后，环境返回的情况。
*   **奖励():**从环境返回给代理的反馈，用于评估代理的动作。
*   **Policy():** Policy 是代理基于当前状态为下一个动作应用的策略。
*   **Value():** 用贴现因子预期长期回报，与短期回报相反。
*   **Q-value():** 大部分与值相似，但多了一个参数作为当前动作(a)。

* * *

## 强化学习的主要特征

*   在 RL 中，不会向代理指示环境和需要采取的操作。
*   它基于命中和审判过程。
*   代理采取下一个动作，并根据前一个动作的反馈改变状态。
*   代理人可能会得到延迟的报酬。
*   环境是随机的，代理人需要探索它才能获得最大的正回报。

* * *

## 强化学习的实施方法

在 ML 中实现强化学习的方法主要有三种，分别是:

1.  **基于价值:**
    基于价值的方法即将找到最优价值函数，这是任何政策下一个状态下的最大值。因此，代理人期望在保单π下任何状态下的长期回报。
2.  **基于策略:**
    基于策略的方法是在不使用价值函数的情况下，寻找未来回报最大化的最优策略。在这种方法中，代理试图应用这样一种策略，即在每个步骤中执行的操作有助于最大化未来的回报。
    基于策略的方法主要有两种策略:
    *   **确定性:**在任何状态下，策略(π)都会产生相同的动作。
    *   **随机性:**在这个策略中，概率决定了产生的动作。
3.  **基于模型:**在基于模型的方法中，为环境创建一个虚拟模型，代理探索该环境来学习它。这种方法没有特定的解决方案或算法，因为每个环境的模型表示是不同的。

* * *

## 强化学习的要素

强化学习有四个主要要素，如下所示:

1.  政策
2.  奖励信号
3.  价值函数
4.  环境模型

**1)策略:**策略可以定义为代理在给定时间的行为方式。它将感知到的环境状态映射到对这些状态采取的行动。策略是 RL 的核心元素，因为只有它可以定义代理的行为。在某些情况下，它可能是一个简单的函数或查找表，而在其他情况下，它可能涉及作为搜索过程的一般计算。它可以是确定性的或随机的策略:

**对于确定性策略:a = π(s)**
**对于随机策略:π(a | s) = P[At =a | St = s]**

**2)奖励信号:**强化学习的目标由奖励信号定义。在每种状态下，环境都会向学习代理发送即时信号，这个信号被称为**奖励信号**。这些奖励是根据代理人采取的好的和坏的行动给予的。代理人的主要目标是最大限度地增加好行为的总回报。奖励信号可以改变策略，例如如果代理选择的动作导致低奖励，则策略可以改变以选择未来的其他动作。

**3)价值函数:**价值函数给出了情况和行动有多好以及代理人可以期待多少报酬的信息。奖励表示每个好的和坏的动作的**即时信号，而价值函数指定**未来的好的状态和动作**。价值函数依赖于奖励，因为没有奖励，就没有价值。评估价值的目标是获得更多的回报。**

**4)模型:**强化学习的最后一个要素是模型，它模仿环境的行为。在模型的帮助下，人们可以推断环境将如何表现。比如，如果给定一个状态和一个动作，那么模型可以预测下一个状态和奖励。

该模型用于规划，这意味着它提供了一种方法，在实际体验这些情况之前，通过考虑所有未来情况来采取行动。借助模型解决 RL 问题的方法**被称为基于模型的方法**。相比之下，不使用模型**的方法**被称为**无模型方法**。****

 *** * *

## 强化学习是如何工作的？

为了理解 RL 的工作过程，我们需要考虑两件主要的事情:

*   **环境:**可以是房间、迷宫、足球场等任何东西。
*   **Agent:**AI 机器人等智能 Agent。

让我们举一个代理需要探索的迷宫环境的例子。请看下图:

![How does Reinforcement Learning Works](../Images/c656fbc38585d3e8313ed3c292bb4a1a.png)

在上图中，代理位于迷宫的第一个街区。迷宫由一个 S <sub>6</sub> 块组成，是一个**墙**，S <sub>8</sub> 一个**火坑**，S <sub>4</sub> 一个**菱形块。**

代理不能越过 S <sub>6</sub> 块，因为它是一堵实心墙。如果代理到达 S <sub>4</sub> 区块，则获得 **+1 奖励；**如果到达火坑，则获得 **-1 奖励点**。它可以采取四个动作**:上移、下移、左移、右移。**

代理可以通过任何途径到达最后一点，但是他需要尽可能少的步骤。假设代理考虑路径 **S9-S5-S1-S2-S3** ，那么他将获得+1 奖励点。

代理将尝试记住它为到达最后一步所采取的前面的步骤。为了记住这些步骤，它会为前面的每个步骤分配一个值。请考虑以下步骤:

![How does Reinforcement Learning Works](../Images/44da7de27b925b337579f871f7597b33.png)

现在，代理已经成功地存储了前面的步骤，将 1 值分配给每个前面的块。但是如果代理从两边都有 1 个值块的块开始移动，他会怎么做呢？考虑下图:

![How does Reinforcement Learning Works](../Images/812f971072a97b5565c2978170ee9951.png)

对于代理来说，他应该上升还是下降将是一个困难的条件，因为每个块都有相同的值。所以，上述方法不适合代理到达目的地。因此，为了解决这个问题，我们将使用**贝尔曼方程**，这是强化学习背后的主要概念。

* * *

## 贝尔曼方程

贝尔曼方程是数学家理查德·欧内斯特·贝尔曼在 1953 年提出的，因此被称为贝尔曼方程。它与动态规划相关联，用于通过包含先前状态的值来计算某一点的决策问题的值。

它是在动态规划或环境中计算价值函数的一种方式，导致了现代强化学习。

贝尔曼方程中使用的关键元素是:

*   代理执行的操作称为“a”
*   执行操作时出现的状态为“s”。
*   对每一个好的和坏的行为获得的奖励/反馈是“r”
*   折扣系数是γ“γ”

贝尔曼方程可以写成:

```
V(s) = max [R(s,a) + γV(s`)]

```

哪里，

**V(s)=在特定点计算的值。**

**R(s，a) =通过执行某个动作在特定状态 s 下的奖励。**

**γ =折扣系数**

**V(s`) =前一状态的值。**

在上面的等式中，我们取完整值的最大值，因为代理总是试图找到最优解。

所以现在，使用贝尔曼方程，我们将在给定环境的每个状态下找到值。我们将从目标区块旁边的区块开始。

**第一个街区:**

V(s3) = max [R(s，a) + γV(s`)]，这里 V(s')= 0，因为没有进一步的状态可以移动。

V(s3)=最大值[R(s，a)]=> V(s3)=最大值[1]=> **V(s3)= 1。**

**第二个街区:**

V(s2) = max [R(s，a) + γV(s`)]，这里γ= 0.9(let)，V(s')= 1，R(s，a)= 0，因为这个状态没有奖励。

V(s2)=最大值[0.9(1)]=> V(s)=最大值[0.9]=> **V(s2) =0.9**

**第三个街区:**

V(s1) = max [R(s，a) + γV(s`)]，这里γ= 0.9(let)，V(s')= 0.9，R(s，a)= 0，因为在这个状态下也没有奖励。

V(s1)=最大值[0.9(0.9)]=> V(s3)=最大值[0.81]=> **V(s1) =0.81**

**第四街区:**

V(s5) = max [R(s，a) + γV(s`)]，这里γ= 0.9(let)，V(s')= 0.81，R(s，a)= 0，因为在这个状态下也没有奖励。

V(s5)=最大值[0.9(0.81)]=> V(s5)=最大值[0.81]=> **V(s5) =0.73**

**第五街区:**

V(s9) = max [R(s，a) + γV(s`)]，这里γ= 0.9(let)，V(s')= 0.73，R(s，a)= 0，因为在这个状态下也没有奖励。

V(s9)=最大值[0.9(0.73)]=> V(s4)=最大值[0.81]=> **V(s4) =0.66**

**考虑下图:**

![Bellman Equation](../Images/ebd3c56416fd44196461cc8b1222e08f.png)

现在，我们将进一步移动到第 6<sup>块，这里代理可能会改变路线，因为它总是试图找到最佳路径。现在，让我们从火坑旁边的街区开始考虑。</sup>

![Bellman Equation](../Images/72b9a142e30c1cd81d4f71773a672507.png)

现在，代理有三个选项可以移动；如果他移动到蓝盒子，那么他会感觉到一个凸起如果他移动到火坑，那么他会得到-1 奖励。但是在这里，我们只获得积极的回报，所以为此，他只会向上。将使用此公式计算完整的块值。请考虑下图:

![Bellman Equation](../Images/fbac2a3d3404459628648e8a69932a89.png)

* * *

## 强化学习的类型

强化学习主要有两种类型，它们是:

*   **正强化**
*   **负筋**

**正强化:**

积极强化学习意味着添加一些东西来增加预期行为再次发生的趋势。它对代理的行为产生积极的影响，并增加行为的强度。

这种强化可以维持变化很长时间，但过多的积极强化可能会导致状态过载，从而减少后果。

**负强化:**

负强化学习与正强化相反，因为它通过避免负条件增加了特定行为再次发生的趋势。

视情况和行为而定，它可能比积极强化更有效，但它提供强化只是为了满足最低限度的行为。

### 如何表示代理状态？

我们可以使用包含历史中所有必需信息的**马尔可夫状态**来表示代理状态。状态 St 是马尔可夫状态，如果它符合给定的条件:

```
P[St+1 | St ] = P[St +1 | S1,......, St]

```

马尔可夫状态遵循**马尔可夫属性**，即未来独立于过去，只能用现在来定义。RL 在完全可观察的环境中工作，其中代理可以观察环境并为新状态采取行动。完整的过程称为马尔可夫决策过程，解释如下:

* * *

## 马尔可夫决策过程

马尔可夫决策过程或 MDP，用于**形式化强化学习问题**。如果环境是完全可观察的，那么它的动态可以建模为**马尔可夫过程**。在 MDP，代理不断与环境交互并执行操作；在每个动作中，环境都会做出响应并生成一个新的状态。

![Markov Decision Process](../Images/612d5000306b7a0b3e37e56555ac3225.png)

MDP 用来描述 RL 的环境，几乎所有的 RL 问题都可以用 MDP 来形式化。

MDP 包含四个元素的元组(S，A，P <sub>a</sub> ，R <sub>a</sub> ):

*   一组有限状态 S
*   一组有限的动作
*   由于行动 a，从状态 S 转换到状态 S '后收到的奖励
*   概率 P <sub>a</sub> 。

MDP 使用马尔可夫属性，为了更好地理解 MDP，我们需要了解它。

### 马尔可夫属性:

它说 ***“如果代理存在于当前状态 s1，执行动作 a1 并移动到状态 s2，那么从 S1 到 s2 的状态转换仅取决于当前状态和未来动作，并且状态不取决于过去的动作、奖励或状态。”*T3】**

或者，换句话说，根据马尔可夫属性，当前状态转换不依赖于任何过去的动作或状态。因此，MDP 问题是一个满足马尔可夫性的 RL 问题。如在**棋类游戏中，玩家只关注当前状态，不需要记住过去的动作或状态**。

**有限 MDP:**

有限的 MDP 是指存在有限的状态、有限的回报和有限的行为。在 RL 中，我们只考虑有限 MDP。

### 马尔可夫过程:

马尔可夫过程是一个无记忆过程，具有一系列随机状态 S <sub>1</sub> ，S <sub>2</sub> ，.....，S <sub>t</sub> 使用马尔可夫属性。马尔可夫过程又称为马尔可夫链，是状态 S 和转移函数 P 上的元组(S，P)，这两个分量(S 和 P)可以定义系统的动力学。

* * *

## 强化学习算法

强化学习算法主要用于 AI 应用和游戏应用。主要使用的算法有:

*   **Q-学习:**
    *   Q-learning 是一种 **Off 策略 RL 算法**，用于时间差异学习。时间差异学习方法是比较时间上连续的预测的方法。
    *   它学习值函数 Q (S，a)，这意味着在特定状态下采取行动“ **a** ”有多好“ **s** ”
    *   下面的流程图解释了 Q- learning 的工作原理:

![Reinforcement Learning Algorithms](../Images/bc40fe1bc77b12dc8414d55a2deb8c68.png)

*   **状态动作奖励状态动作(SARSA):**
    *   SARSA 代表**状态动作奖励状态动作**，是一种 **on-policy** 时间差异学习方法。策略控制方法在使用特定策略学习时为每个状态选择操作。
    *   SARSA 的目标是计算所选当前策略π和所有(s-a)对的 **Q π (s，a)。**
    *   Q 学习和 SARSA 算法的主要区别在于**与 Q 学习不同，更新表中 Q 值不需要下一个状态的最大奖励。**
    *   在 SARSA 中，使用相同的策略选择新的动作和奖励，该策略已经确定了原始动作。
    *   SARSA 之所以被命名，是因为它使用了五元组 **Q(s，a，r，s’，a’)。**所在，
        T3】s:原始状态 T5**a:原始动作**T8**r:奖励在跟随状态**T11**s 和 a 时观察到的:新状态，动作对。**
*   **深度 Q 神经网络(DQN):**
    *   顾名思义，DQN 是一个使用神经网络进行 Q 学习的人。
    *   对于一个大的状态空间环境，定义和更新 Q 表将是一项具有挑战性和复杂性的任务。
    *   为了解决这样的问题，我们可以使用 DQN 算法。其中，神经网络不是定义 Q 表，而是近似每个动作和状态的 Q 值。

现在，我们将扩展 Q 学习。

### 问-学习解释:

*   q 学习是一种流行的基于贝尔曼方程的无模型强化学习算法。
*   **Q-learning 的主要目标是学习策略，该策略可以告知代理在什么情况下应该采取什么行动来实现回报最大化。**
*   这是一个**非策略 RL** 试图找到在当前状态下采取的最佳行动。
*   Q 学习中主体的目标是 Q 的价值最大化。
*   Q 学习的价值可以从贝尔曼方程推导出来。考虑下面给出的贝尔曼方程:

![Q-Learning Explanation](../Images/2d28e05e1063d1369fd04f33606498ab.png)

在等式中，我们有各种成分，包括奖励、折扣因子(γ)、概率和结束状态 s’。但是没有给出任何 Q 值，所以首先考虑下图:

![Q-Learning Explanation](../Images/4ef0ee85bb1cc4bef13f7431c3ffb9fd.png)

在上图中，我们可以看到有一个代理有三个值选项，V(s <sub>1</sub> )，V(s <sub>2</sub> ，V(s <sub>3</sub> )。因为这里是 MDP，所以经纪人只关心现在的状态和未来的状态。代理可以去任何方向(向上、向左或向右)，所以他需要决定去哪里寻找最佳路径。在这里，代理将根据概率采取行动并改变状态。但是如果我们想要一些精确的移动，那么为此，我们需要在 Q 值方面做一些改变。请看下图:

![Q-Learning Explanation](../Images/94a44d3b575e6b2b7c755974df806a02.png)

Q-代表每个状态下的动作质量。因此，我们将使用一对状态和动作，即 Q(s，a)，而不是在每个状态使用一个值。Q 值指定哪个动作比其他动作更润滑，根据最佳 Q 值，代理采取下一步动作。贝尔曼方程可用于推导 Q 值。

为了执行任何动作，代理将获得一个奖励 R(s，a)，并且他将结束于某个状态，因此 Q 值等式将是:

![Q-Learning Explanation](../Images/d0b7470864066d44dc82a8f5beda9d14.png)

因此，我们可以说， ***V(s) = max [Q(s，a)]***

![Q-Learning Explanation](../Images/ca99b5b6fa31feb2614eb46b1b26f937.png)

**上述公式用于估计 Q 学习中的 Q 值。**

**问学中的“问”是什么？**

在 **Q-learning** 中，Q 代表**质量**，这意味着它指定了代理采取的行动的质量。

### q 表:

在进行 Q 学习时，会创建一个 Q 表或矩阵。该表遵循状态和动作对，即[s，a]，并将值初始化为零。每次操作后，表都会更新，q 值存储在表中。

RL 代理使用这个 Q 表作为参考表，根据 Q 值选择最佳操作。

* * *

## 强化学习和监督学习的区别

强化学习和监督学习都是机器学习的一部分，但这两种学习方式截然相反。RL 代理与环境互动，探索环境，采取行动，并获得回报。而监督学习算法从标记的数据集学习，并在训练的基础上预测输出。

反向学习和监督学习之间的差异表如下:

| 强化学习 | 监督学习 |
| RL 通过与环境交互来工作。 | 监督学习对现有数据集有效。 |
| RL 算法的工作原理就像人脑在做一些决定时的工作原理一样。 | 监督学习就像人类在向导的监督下学习一样。 |
| 不存在带标签的数据集 | 已标记的数据集存在。 |
| 没有向学习代理提供以前的培训。 | 向算法提供训练，以便它能够预测输出。 |
| RL 有助于按顺序做出决策。 | 在监督学习中，当给定输入时做出决定。 |

* * *

## 强化学习的应用

![Reinforcement Learning Applications](../Images/c7693418221b88e241c326bcae9205c3.png)

1.  **机器人技术:**
    1.  RL 用于**机器人导航、机器人足球、行走、杂耍**等。
2.  **控制:**
    1.  RL 可用于**自适应控制**，如工厂流程、电信中的准入控制，直升机飞行员就是强化学习的一个例子。
3.  **游戏玩法:**
    1.  RL 可用于**游戏玩**如井字游戏、国际象棋等。
4.  **化学:**
    1.  RL 可用于优化化学反应。
5.  **业务:**
    1.  RL 现在用于商业战略规划。
6.  **制造:**
    1.  在各种汽车制造公司，机器人使用深度强化学习来挑选货物并将其放入一些容器中。
7.  **金融板块:**
    1.  RL 目前在金融领域用于评估交易策略。

* * *

## 结论:

从上面的讨论中，我们可以说强化学习是机器学习中最有趣和最有用的部分之一。在 RL 中，代理在没有任何人工干预的情况下通过探索环境来探索环境。它是人工智能中使用的主要学习算法。但也有一些不应该使用的情况，比如如果你有足够的数据来解决问题，那么其他 ML 算法可以更高效地使用。RL 算法的主要问题是一些参数可能会影响学习速度，例如延迟反馈。

* * ***