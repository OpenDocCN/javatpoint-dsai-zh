# 决策树归纳

> 原文:[https://www.javatpoint.com/decision-tree-induction](https://www.javatpoint.com/decision-tree-induction)

决策树是一种有监督的学习方法，用于数据挖掘中的分类和回归方法。这是一棵帮助我们做出决策的树。决策树以树形结构创建分类或回归模型。它将数据集分成更小的子集，同时决策树也在稳步发展。最终的树是具有决策节点和叶节点的树。决策节点至少有两个分支。叶节点显示分类或决策。我们不能在叶节点上完成更多的分割——树中最上面的决策节点，它与被称为根节点的最佳预测器相关。决策树可以处理分类数据和数字数据。

## 关键因素:

### 熵:

熵是衡量杂质的常用方法。在决策树中，它衡量数据集中的随机性或不确定性。

![Decision Tree Induction](../Images/273623dc092b72f7b0fa04acdacf2d1f.png)

### 信息增益:

信息增益是指数据集分割后熵的下降。也叫**熵减**。构建决策树就是要发现能够获得最高数据收益的属性。

![Decision Tree Induction](../Images/e77ff6170780f724bac5ee5d064db817.png)

简而言之，决策树就像一个流程图，终端节点显示决策。从数据集开始，我们可以测量熵来找到一种方法来分割数据集，直到数据属于同一个类。

## 决策树为什么有用？

它使我们能够彻底分析一个决定的可能后果。

它为我们提供了一个框架来衡量结果的价值和实现它们的可能性。

它帮助我们根据现有数据和最佳推测做出最佳决策。

**换句话说**，我们可以说决策树是一种层次树结构，可以通过实现一系列简单的决策规则，将大量的记录集合拆分成更小的类集合。决策树模型由一组规则组成，用于将一个巨大的异质群体分成更小、更同质或互斥的类别。类的属性可以是任何标称值、序数、二进制值和定量值的变量，相反，类必须是定性类型，如分类或序数或二进制。简而言之，给定的属性数据及其类别，决策树创建了一组规则，可用于识别类别。一个规则被一个接一个地实现，从而在一个段中产生段的层次结构。层次结构被称为**树**，每个片段被称为一个**节点**。随着每次渐进的划分，后续集合中的成员变得越来越相似。因此，用于构建决策树的算法被称为递归划分。该算法被称为 **CART** (分类和回归树)

考虑给定的工厂示例，其中

![Decision Tree Induction](../Images/c2e38e17a51d470c32b7fb724672e8e5.png)

扩大要素成本为 300 万美元，经济好的概率为 0.6 (60%)，带来 800 万美元利润，经济不好的概率为 0.4 (40%)，带来 600 万美元利润。

成本为 0 美元的非扩张因素，经济好的概率为 0.6(60%)，导致 400 万美元的利润，经济不好的概率为 0.4，导致 200 万美元的利润。

管理团队需要根据给定的数据做出是否扩展的数据驱动决策。

净扩张= ( 0.6 *8 + 0.4*6 ) - 3 = $4.2M
净不扩张=(0.6 * 4+0.4 * 2)-0 = $ 3M
$ 4.2M>$ 3M，因此工厂应该扩张。

## 决策树算法:

决策树算法可能看起来很长，但它非常简单，其基本算法技术如下:

**算法**基于三个参数 **: D，属性 _ 列表，属性** _ **选择 _ 方法。**

通常，我们将 **D** 称为**数据分区。**

最初， **D** 是整个**训练元组**及其相关的**类级别**的集合(输入训练数据)。

参数**属性列表**是定义元组的一组**属性**。

**Attribute _ selection _ method**指定一个**启发式过程**，用于根据**类**选择“最佳”区分给定元组的属性。

**属性选择方法**过程应用**属性选择度量**。

## 使用决策树的优势:

决策树不需要缩放信息。

数据中缺失的值也不会在很大程度上影响构建选择树的过程。

决策树模型是自动的，并且易于向技术团队和利益相关者解释。

与其他算法相比，决策树在预处理过程中需要较少的数据准备工作。

决策树不需要数据标准化。

* * *